{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Training models in Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* two step proces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we will use the `fit()` method to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* before we **fit** a model, we must **compile it**\n",
    "\n",
    "    * this will allow us to decide which optimizer, which loss function and which metrics to use for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Compiling models in Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* performed using the `compile()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* additional information needed before training our neural network (up until now we have only defined the structure of our network and nothing else)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* when compiling models, there are two parameters we must  define:\n",
    "\n",
    "    * **`optimizer`**\n",
    "    * **`loss function`**\n",
    "\n",
    "\n",
    "* it is also a good idea to always define the **`metrics`** parameter\n",
    "    * it allows us to specify which metrics should the model evaluate during training and testing\n",
    "    \n",
    "    \n",
    "* other parameters we can set **(for beginners it is best if they just leave them at default values)**:\n",
    "\n",
    "    * **loss_weights** \n",
    "    * **weighted_metrics** \n",
    "    * **run_eagerly** \n",
    "    * **kwargs** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Optimizing models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* to optimize neural networks we use a technique known as gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* a generic optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* used to optimize neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it  starts with some random values for each weight $w_i$ and iteratively adjusts these values in order to minimize a **`cost function`**  until the algorithm **converges to a minimum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* an important parameter that modifies gradient descent is the **learning rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://edlitera-images.s3.amazonaws.com/gradient_descent.png\" width=\"700\">\n",
    "\n",
    "source:\n",
    "<br>\n",
    "https://morioh.com/p/15c995420be6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `Learning rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* denoted as $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* varies between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* determines the magnitude of changes for each of the model parameters in each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* if it's too small, it will take a long time for gradient descent to converge to a minimum (to get to the bottom)\n",
    "\n",
    "\n",
    "* if it's too large, gradient descent might keep missing the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Gradient Descent variants:**\n",
    "\n",
    "* `Batch Gradient Descent`\n",
    "* `Stochastic Gradient Descent`\n",
    "* `Mini-batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note:** Mini-batch Gradient Descent is the one that is tipically used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `Mini-batch Gradient Descent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it's a middle ground between Batch Gradient Descent and Stochastic Gradient Descent\n",
    "    * **\"the best of both worlds\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it computes the gradients using a small random subset of the training instances (these are called **mini batches**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* provides a variety of advantages:\n",
    "    * reduced noise (we accumulate gradients from multiple training examples)\n",
    "    * efficiency\n",
    "    * fast convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**We need to choose the right batch size: that size becomes a hyperparameter of the model**\n",
    "\n",
    "* small batch size: converges more quickly, but is less accurate\n",
    "\n",
    "* big batch size: converges slower, but is more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Hardware limitations**\n",
    "\n",
    "* we prefer to train Deep Learning models on GPUs\n",
    "\n",
    "* because of that, we aim to pick batch sizes that are powers of two (8, 16, 32)\n",
    "\n",
    "* bigger batch sizes also require more memory so take that into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it comes with its own set of problems:\n",
    "\n",
    "    * hard to choose a good learning rate\n",
    "    * it is easy to get trapped in a local minima spot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://edlitera-images.s3.amazonaws.com/local_minima_problem.png\" width=\"700\">\n",
    "\n",
    "source:\n",
    "<br>\n",
    "https://medium.com/@raza.shan83/gradient-descent-c568801d0b62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Gradient descent optimization algorithms`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Optimization algorithms:**\n",
    "\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* RMSprop\n",
    "* Adam\n",
    "\n",
    "**Honourable mentions:**\n",
    "\n",
    "* AdaMax\n",
    "* Nadam\n",
    "* AMSGrad\n",
    "* AdamW\n",
    "* QHAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Gradient descent optimization algorithms in Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* list of optimizers available at: https://keras.io/api/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Keras supports the following optimizers\n",
    "    <br>\n",
    "    \n",
    "    * SGD\n",
    "    * RMSprop\n",
    "    * Adam\n",
    "    * Adadelta\n",
    "    * Adagrad\n",
    "    * Adamax\n",
    "    * Nadam\n",
    "    * Ftrl\n",
    "    \n",
    "    \n",
    "* most of the time you will use one of the more popular optimizers: **SGD, RMSprop or Adam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# Define example model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation=\"relu\", input_shape=(8,)))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Define optimizer and loss function\n",
    "\n",
    "optim = Adam(learning_rate=0.2)\n",
    "\n",
    "loss_function = CategoricalCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Loss functions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* optimizers such as Adam try to minimize error in the algorithm\n",
    "\n",
    "* the error they are trying to minimize is computed using loss functions \n",
    "    * they allow us to quantify how well a model is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Loss functions used in classification problems:** \n",
    "\n",
    "* `Binary Cross-Entropy`\n",
    "* `Categorical Cross-Entropy`\n",
    "* `Sparse Categorical Cross-Entropy`\n",
    "\n",
    "**Honourable mentions:**\n",
    "\n",
    "* `Hinge Loss`\n",
    "* `KL Loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Loss functions in Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* list of losses available at: https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* for simple classification tasks you will mostly use:\n",
    "    <br>\n",
    "    \n",
    "    * **binary crossentropy** for binary classification problems\n",
    "    * **categorical crossentropy** for multiclass classification problems\n",
    "    * **sparse categorical crossentropy** - variant of categorical crossentropy that is more efficient to use than categorical crossentropy \n",
    "        * we will use this for multiclass classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "\n",
    "optim = Adam()\n",
    "loss_function = SparseCategoricalCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* list of metrics available at: https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* divided into:\n",
    "    <br>\n",
    "    \n",
    "    * Accuracy metrics\n",
    "    * Probabilistic metrics\n",
    "    * Regression metrics\n",
    "    * Classification metrics based on True/False positives & negatives\n",
    "    * etc.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* used to judge the performance of the model\n",
    "\n",
    "\n",
    "* keep in mind that any loss function can also be used as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Define a loss function, an optimizer and a metric\n",
    "\n",
    "loss_function = SparseCategoricalCrossentropy()\n",
    "\n",
    "metric = SparseCategoricalAccuracy()\n",
    "\n",
    "optim = Adam()\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optim, metrics=[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Fitting models in Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* performed using the `fit()` method\n",
    "    * the model is trained for a certain number of epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Arguments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`x`**\n",
    "\n",
    "- input data used for predicting target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`y`**\n",
    "\n",
    "- target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`batch_size`** \n",
    "\n",
    "- the size of the minibatches (**remember:** use multiples of 2)\n",
    "\n",
    "- **NOTE:** no need to use this if you use a generator or something similar to feed data into your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`epochs`** \n",
    "\n",
    "- how many epochs to train the model for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`verbose`** \n",
    "\n",
    "- verbosity mode, you can leave it on \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`callbacks`** \n",
    "\n",
    "- the progress bar logger and the history callbacks get created automatically, but all the others need to be specified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`validation_split`** \n",
    "\n",
    "- how much of the data to use for the validation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`validation_data`**\n",
    "\n",
    "- if you have separated data for validation, pass it using this argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`shuffle`**\n",
    "\n",
    "- `True` if you want to shuffle the data before each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`validation_batch_size`**  \n",
    "\n",
    "- the size of the validation data minibatch (remember: use multiples of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import BinaryAccuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_study</th>\n",
       "      <th>monthly_tuition</th>\n",
       "      <th>passing_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   daily_study  monthly_tuition  passing_grade\n",
       "0            7               27              1\n",
       "1            2               43              0\n",
       "2            7               26              1\n",
       "3            8               29              1\n",
       "4            3               42              0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "# Display first five rows\n",
    "\n",
    "df = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/students.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Separate features from the label\n",
    "\n",
    "X = df[[\"daily_study\", \"monthly_tuition\"]]\n",
    "\n",
    "# Flatten data\n",
    "\n",
    "y = df[\"passing_grade\"]\n",
    "y = y.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split train data into train and validation data\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "xtest_cp = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define scaler and scale data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Define input dimension\n",
    "\n",
    "input_dimension = X_train.shape[1]\n",
    "\n",
    "print(input_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 8)                 24        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation=\"relu\", input_shape=(input_dimension,)))\n",
    "\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "\n",
    "optim = Adam()\n",
    "\n",
    "# Define loss\n",
    "\n",
    "loss_function = BinaryCrossentropy()\n",
    "\n",
    "# Define metric we will track \n",
    "\n",
    "metric = BinaryAccuracy()\n",
    "\n",
    "# Compile model\n",
    "\n",
    "model.compile(\n",
    "    loss=loss_function,\n",
    "    optimizer=optim,\n",
    "    metrics=[metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 17ms/step - loss: 0.6445 - binary_accuracy: 0.7469 - val_loss: 0.6347 - val_binary_accuracy: 0.7095\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6265 - binary_accuracy: 0.7571 - val_loss: 0.6165 - val_binary_accuracy: 0.7238\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6083 - binary_accuracy: 0.8061 - val_loss: 0.5961 - val_binary_accuracy: 0.8429\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5863 - binary_accuracy: 0.8694 - val_loss: 0.5734 - val_binary_accuracy: 0.8476\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5620 - binary_accuracy: 0.8837 - val_loss: 0.5477 - val_binary_accuracy: 0.8714\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5349 - binary_accuracy: 0.8816 - val_loss: 0.5193 - val_binary_accuracy: 0.8619\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5057 - binary_accuracy: 0.8837 - val_loss: 0.4904 - val_binary_accuracy: 0.8619\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4753 - binary_accuracy: 0.8918 - val_loss: 0.4603 - val_binary_accuracy: 0.8714\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4441 - binary_accuracy: 0.8959 - val_loss: 0.4307 - val_binary_accuracy: 0.8714\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4136 - binary_accuracy: 0.8980 - val_loss: 0.4012 - val_binary_accuracy: 0.8762\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3835 - binary_accuracy: 0.9102 - val_loss: 0.3731 - val_binary_accuracy: 0.8952\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3547 - binary_accuracy: 0.9122 - val_loss: 0.3472 - val_binary_accuracy: 0.8952\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3280 - binary_accuracy: 0.9143 - val_loss: 0.3226 - val_binary_accuracy: 0.9048\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3039 - binary_accuracy: 0.9184 - val_loss: 0.3009 - val_binary_accuracy: 0.9095\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2817 - binary_accuracy: 0.9265 - val_loss: 0.2813 - val_binary_accuracy: 0.9238\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2624 - binary_accuracy: 0.9367 - val_loss: 0.2639 - val_binary_accuracy: 0.9381\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2454 - binary_accuracy: 0.9408 - val_loss: 0.2487 - val_binary_accuracy: 0.9381\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2301 - binary_accuracy: 0.9449 - val_loss: 0.2357 - val_binary_accuracy: 0.9429\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2172 - binary_accuracy: 0.9449 - val_loss: 0.2237 - val_binary_accuracy: 0.9429\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2056 - binary_accuracy: 0.9469 - val_loss: 0.2134 - val_binary_accuracy: 0.9524\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1956 - binary_accuracy: 0.9510 - val_loss: 0.2047 - val_binary_accuracy: 0.9524\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1870 - binary_accuracy: 0.9510 - val_loss: 0.1970 - val_binary_accuracy: 0.9524\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1797 - binary_accuracy: 0.9510 - val_loss: 0.1901 - val_binary_accuracy: 0.9524\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1731 - binary_accuracy: 0.9510 - val_loss: 0.1841 - val_binary_accuracy: 0.9524\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1675 - binary_accuracy: 0.9551 - val_loss: 0.1788 - val_binary_accuracy: 0.9571\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1624 - binary_accuracy: 0.9612 - val_loss: 0.1741 - val_binary_accuracy: 0.9571\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1578 - binary_accuracy: 0.9612 - val_loss: 0.1700 - val_binary_accuracy: 0.9571\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1538 - binary_accuracy: 0.9612 - val_loss: 0.1662 - val_binary_accuracy: 0.9571\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1502 - binary_accuracy: 0.9612 - val_loss: 0.1622 - val_binary_accuracy: 0.9571\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1469 - binary_accuracy: 0.9612 - val_loss: 0.1589 - val_binary_accuracy: 0.9571\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1438 - binary_accuracy: 0.9592 - val_loss: 0.1560 - val_binary_accuracy: 0.9619\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1414 - binary_accuracy: 0.9612 - val_loss: 0.1532 - val_binary_accuracy: 0.9619\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1387 - binary_accuracy: 0.9612 - val_loss: 0.1505 - val_binary_accuracy: 0.9619\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1365 - binary_accuracy: 0.9612 - val_loss: 0.1479 - val_binary_accuracy: 0.9619\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1343 - binary_accuracy: 0.9612 - val_loss: 0.1457 - val_binary_accuracy: 0.9619\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1324 - binary_accuracy: 0.9612 - val_loss: 0.1436 - val_binary_accuracy: 0.9619\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1306 - binary_accuracy: 0.9612 - val_loss: 0.1417 - val_binary_accuracy: 0.9619\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1290 - binary_accuracy: 0.9633 - val_loss: 0.1400 - val_binary_accuracy: 0.9619\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1274 - binary_accuracy: 0.9653 - val_loss: 0.1382 - val_binary_accuracy: 0.9619\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1261 - binary_accuracy: 0.9653 - val_loss: 0.1364 - val_binary_accuracy: 0.9619\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1244 - binary_accuracy: 0.9673 - val_loss: 0.1349 - val_binary_accuracy: 0.9619\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1232 - binary_accuracy: 0.9673 - val_loss: 0.1335 - val_binary_accuracy: 0.9619\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1220 - binary_accuracy: 0.9673 - val_loss: 0.1323 - val_binary_accuracy: 0.9619\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1203 - binary_accuracy: 0.9673 - val_loss: 0.1307 - val_binary_accuracy: 0.9619\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1188 - binary_accuracy: 0.9673 - val_loss: 0.1295 - val_binary_accuracy: 0.9667\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1172 - binary_accuracy: 0.9673 - val_loss: 0.1282 - val_binary_accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1159 - binary_accuracy: 0.9673 - val_loss: 0.1269 - val_binary_accuracy: 0.9667\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1146 - binary_accuracy: 0.9673 - val_loss: 0.1256 - val_binary_accuracy: 0.9667\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1133 - binary_accuracy: 0.9673 - val_loss: 0.1243 - val_binary_accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1121 - binary_accuracy: 0.9673 - val_loss: 0.1230 - val_binary_accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1110 - binary_accuracy: 0.9673 - val_loss: 0.1219 - val_binary_accuracy: 0.9667\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1100 - binary_accuracy: 0.9673 - val_loss: 0.1210 - val_binary_accuracy: 0.9667\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1093 - binary_accuracy: 0.9673 - val_loss: 0.1198 - val_binary_accuracy: 0.9714\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1086 - binary_accuracy: 0.9694 - val_loss: 0.1191 - val_binary_accuracy: 0.9714\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1077 - binary_accuracy: 0.9694 - val_loss: 0.1182 - val_binary_accuracy: 0.9714\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1070 - binary_accuracy: 0.9694 - val_loss: 0.1175 - val_binary_accuracy: 0.9714\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1064 - binary_accuracy: 0.9694 - val_loss: 0.1168 - val_binary_accuracy: 0.9714\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1056 - binary_accuracy: 0.9694 - val_loss: 0.1161 - val_binary_accuracy: 0.9714\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1052 - binary_accuracy: 0.9694 - val_loss: 0.1158 - val_binary_accuracy: 0.9714\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1044 - binary_accuracy: 0.9694 - val_loss: 0.1147 - val_binary_accuracy: 0.9714\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1040 - binary_accuracy: 0.9694 - val_loss: 0.1140 - val_binary_accuracy: 0.9714\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1032 - binary_accuracy: 0.9694 - val_loss: 0.1134 - val_binary_accuracy: 0.9714\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1026 - binary_accuracy: 0.9694 - val_loss: 0.1128 - val_binary_accuracy: 0.9714\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1020 - binary_accuracy: 0.9694 - val_loss: 0.1123 - val_binary_accuracy: 0.9714\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1019 - binary_accuracy: 0.9694 - val_loss: 0.1114 - val_binary_accuracy: 0.9762\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1009 - binary_accuracy: 0.9694 - val_loss: 0.1110 - val_binary_accuracy: 0.9762\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1004 - binary_accuracy: 0.9694 - val_loss: 0.1104 - val_binary_accuracy: 0.9762\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1001 - binary_accuracy: 0.9694 - val_loss: 0.1099 - val_binary_accuracy: 0.9762\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0995 - binary_accuracy: 0.9694 - val_loss: 0.1093 - val_binary_accuracy: 0.9762\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0989 - binary_accuracy: 0.9694 - val_loss: 0.1088 - val_binary_accuracy: 0.9762\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0984 - binary_accuracy: 0.9694 - val_loss: 0.1083 - val_binary_accuracy: 0.9762\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0979 - binary_accuracy: 0.9694 - val_loss: 0.1079 - val_binary_accuracy: 0.9762\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0974 - binary_accuracy: 0.9694 - val_loss: 0.1073 - val_binary_accuracy: 0.9762\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0967 - binary_accuracy: 0.9694 - val_loss: 0.1069 - val_binary_accuracy: 0.9810\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0962 - binary_accuracy: 0.9714 - val_loss: 0.1064 - val_binary_accuracy: 0.9810\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0959 - binary_accuracy: 0.9714 - val_loss: 0.1059 - val_binary_accuracy: 0.9810\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0956 - binary_accuracy: 0.9714 - val_loss: 0.1054 - val_binary_accuracy: 0.9810\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0955 - binary_accuracy: 0.9714 - val_loss: 0.1054 - val_binary_accuracy: 0.9810\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0943 - binary_accuracy: 0.9735 - val_loss: 0.1045 - val_binary_accuracy: 0.9810\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0943 - binary_accuracy: 0.9776 - val_loss: 0.1040 - val_binary_accuracy: 0.9810\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0937 - binary_accuracy: 0.9776 - val_loss: 0.1037 - val_binary_accuracy: 0.9810\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0934 - binary_accuracy: 0.9755 - val_loss: 0.1034 - val_binary_accuracy: 0.9810\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0931 - binary_accuracy: 0.9714 - val_loss: 0.1031 - val_binary_accuracy: 0.9810\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0927 - binary_accuracy: 0.9714 - val_loss: 0.1030 - val_binary_accuracy: 0.9810\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0924 - binary_accuracy: 0.9714 - val_loss: 0.1026 - val_binary_accuracy: 0.9810\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0923 - binary_accuracy: 0.9735 - val_loss: 0.1023 - val_binary_accuracy: 0.9810\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0917 - binary_accuracy: 0.9776 - val_loss: 0.1020 - val_binary_accuracy: 0.9810\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0915 - binary_accuracy: 0.9776 - val_loss: 0.1017 - val_binary_accuracy: 0.9810\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0912 - binary_accuracy: 0.9776 - val_loss: 0.1015 - val_binary_accuracy: 0.9810\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0909 - binary_accuracy: 0.9776 - val_loss: 0.1012 - val_binary_accuracy: 0.9810\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0913 - binary_accuracy: 0.9714 - val_loss: 0.1017 - val_binary_accuracy: 0.9810\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0905 - binary_accuracy: 0.9714 - val_loss: 0.1014 - val_binary_accuracy: 0.9810\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0901 - binary_accuracy: 0.9776 - val_loss: 0.1009 - val_binary_accuracy: 0.9810\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0898 - binary_accuracy: 0.9776 - val_loss: 0.1007 - val_binary_accuracy: 0.9810\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0900 - binary_accuracy: 0.9776 - val_loss: 0.1004 - val_binary_accuracy: 0.9810\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0894 - binary_accuracy: 0.9776 - val_loss: 0.1003 - val_binary_accuracy: 0.9810\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0893 - binary_accuracy: 0.9776 - val_loss: 0.1001 - val_binary_accuracy: 0.9810\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0889 - binary_accuracy: 0.9776 - val_loss: 0.1001 - val_binary_accuracy: 0.9810\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0887 - binary_accuracy: 0.9776 - val_loss: 0.1000 - val_binary_accuracy: 0.9810\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0885 - binary_accuracy: 0.9776 - val_loss: 0.0998 - val_binary_accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ac178ef70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_valid, y_valid), \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Evaluating models in Keras and making predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* by evaluating we analyze the performance of our model\n",
    "    * returns the loss value\n",
    "    * returns what we specified as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* performed using the `evalute()` method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* evaluation is performed in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Arguments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`x`**\n",
    "\n",
    "- input data used for predicting target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`y`**\n",
    "\n",
    "- target data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`batch_size`** \n",
    "\n",
    "- the size of the minibatches (**remember:** use multiples of 2)\n",
    "\n",
    "- **NOTE:** no need to use this if you use a generator or something similar to feed data into your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`verbose`** \n",
    "\n",
    "- verbosity mode, you can leave it on \"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`callbacks`** \n",
    "\n",
    "- the callbacks we want to apply during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`steps`** \n",
    "\n",
    "- number of batches before we define that an evaluation round has ended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Making predictions using trained models in Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* once we have a trained model, we can use it to generate predictions\n",
    "    * computation for this is done in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* to generate predictions for some input samples we use the `predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Arguments`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`x`**\n",
    "\n",
    "- input samples for which we want to generate predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`batch_size`** \n",
    "\n",
    "- the size of the minibatches (**remember:** use multiples of 2)\n",
    "\n",
    "- **NOTE:** no need to use this if you use a generator or something similar to feed data into your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`verbose`** \n",
    "\n",
    "- verbosity mode, you need to select 0 or 1\n",
    "\n",
    "- 0 will make sure that nothing is showed on the screen while making predictions\n",
    "\n",
    "- 1 will display progress on screen when making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`steps`** \n",
    "\n",
    "- number of batches before we define that a prediction round has ended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`callbacks`** \n",
    "\n",
    "- the callbacks we want to apply during prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.53629948  0.01125812]\n",
      " [-0.37918842  0.34559009]\n",
      " [-2.29467632  1.34858599]\n",
      " [ 0.09968355 -0.8245718 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.37918842  0.84708804]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  1.51575198]\n",
      " [ 0.57855553  0.67992205]\n",
      " [-1.33693237  1.51575198]\n",
      " [-1.81580434  1.01425402]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.09968355 -0.15590787]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 2.01517145  0.34559009]\n",
      " [-2.29467632  1.51575198]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.37918842  0.67992205]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-1.33693237  1.85008394]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-0.37918842 -1.66040172]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-0.85806039  0.84708804]\n",
      " [-0.37918842  0.67992205]\n",
      " [-0.37918842  0.51275607]\n",
      " [-1.33693237  1.51575198]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-1.33693237  1.68291796]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-1.33693237  1.18142001]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [-1.33693237  2.3515819 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [-1.81580434  2.01724993]\n",
      " [ 0.57855553  0.34559009]\n",
      " [-0.37918842  1.01425402]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.81580434  1.85008394]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  1.51575198]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 0.09968355  0.51275607]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.85806039  1.85008394]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-1.81580434  2.01724993]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-1.33693237  0.34559009]\n",
      " [-1.33693237  0.51275607]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [-1.81580434  1.51575198]\n",
      " [ 0.09968355  1.01425402]\n",
      " [-1.33693237  1.34858599]\n",
      " [-1.33693237  1.51575198]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.37918842 -0.65740582]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.09968355  1.01425402]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.09968355  0.34559009]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.37918842  0.34559009]\n",
      " [-2.29467632  1.68291796]\n",
      " [-0.85806039  0.01125812]\n",
      " [-0.37918842  0.84708804]\n",
      " [-1.33693237  0.51275607]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-0.37918842  0.34559009]\n",
      " [ 1.0574275  -0.8245718 ]\n",
      " [-0.37918842  0.01125812]\n",
      " [-1.33693237  1.51575198]\n",
      " [-1.81580434  1.51575198]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-1.33693237  1.85008394]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-1.81580434  2.3515819 ]\n",
      " [-1.81580434  0.51275607]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.37918842  0.67992205]\n",
      " [-0.85806039  0.34559009]\n",
      " [-0.37918842  1.51575198]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.09968355  0.1784241 ]\n",
      " [ 0.57855553 -0.49023984]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.85806039  1.34858599]\n",
      " [-0.37918842 -0.32307385]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.09968355  0.34559009]\n",
      " [-0.85806039  1.18142001]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.09968355 -0.8245718 ]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [-0.37918842  0.84708804]\n",
      " [-0.85806039  1.18142001]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [-1.81580434  1.34858599]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.85806039  0.51275607]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-1.33693237  1.68291796]\n",
      " [-2.29467632  1.51575198]\n",
      " [-1.33693237  0.34559009]\n",
      " [-0.37918842  0.01125812]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-1.81580434  2.18441591]\n",
      " [-2.29467632  1.18142001]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  0.84708804]\n",
      " [-0.37918842 -0.49023984]\n",
      " [-1.33693237  2.01724993]\n",
      " [-1.33693237  1.34858599]\n",
      " [-2.29467632  1.85008394]\n",
      " [-1.81580434  1.51575198]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 0.09968355 -0.8245718 ]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.85806039  0.34559009]\n",
      " [-0.37918842  0.51275607]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [-0.37918842 -1.82756771]\n",
      " [-0.37918842  1.34858599]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [ 2.01517145  0.34559009]\n",
      " [-0.37918842  0.34559009]\n",
      " [-0.37918842  0.84708804]\n",
      " [-2.29467632  1.18142001]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.49023984]\n",
      " [ 0.09968355 -1.66040172]\n",
      " [-0.37918842  0.34559009]\n",
      " [-0.37918842  1.18142001]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-0.85806039  1.51575198]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.81580434  0.67992205]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [-2.29467632  1.34858599]\n",
      " [-1.81580434  1.68291796]\n",
      " [-1.33693237  1.01425402]\n",
      " [-0.85806039  0.67992205]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [-0.85806039  2.01724993]\n",
      " [-1.81580434  1.68291796]\n",
      " [-1.33693237  1.68291796]\n",
      " [-2.29467632  2.68591386]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553  0.34559009]\n",
      " [-0.85806039  0.1784241 ]\n",
      " [ 0.09968355  0.01125812]\n",
      " [-0.85806039  0.67992205]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.37918842  0.34559009]\n",
      " [ 0.09968355  0.84708804]\n",
      " [-2.77354829  0.34559009]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.37918842 -0.49023984]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [-0.85806039  0.34559009]\n",
      " [-0.37918842  0.34559009]\n",
      " [-1.33693237  0.1784241 ]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [-1.33693237  1.51575198]\n",
      " [-0.85806039  0.51275607]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-1.33693237  1.85008394]\n",
      " [-0.37918842 -0.65740582]\n",
      " [-0.37918842  0.67992205]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [-0.37918842  0.34559009]\n",
      " [-0.85806039  1.68291796]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-0.85806039  1.34858599]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [-0.85806039 -0.15590787]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.85806039  0.01125812]\n",
      " [-0.37918842  1.01425402]\n",
      " [ 0.57855553  0.1784241 ]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-1.81580434  1.34858599]\n",
      " [ 0.09968355  1.01425402]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.33693237  1.68291796]\n",
      " [-0.85806039  0.84708804]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [ 1.0574275   0.34559009]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355  0.01125812]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 1.0574275  -0.49023984]]\n",
      "[[ 1.53629948  0.01125812]\n",
      " [-0.37918842  0.34559009]\n",
      " [-2.29467632  1.34858599]\n",
      " [ 0.09968355 -0.8245718 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.37918842  0.84708804]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  1.51575198]\n",
      " [ 0.57855553  0.67992205]\n",
      " [-1.33693237  1.51575198]\n",
      " [-1.81580434  1.01425402]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.09968355 -0.15590787]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 2.01517145  0.34559009]\n",
      " [-2.29467632  1.51575198]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.37918842  0.67992205]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-1.33693237  1.85008394]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-0.37918842 -1.66040172]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-0.85806039  0.84708804]\n",
      " [-0.37918842  0.67992205]\n",
      " [-0.37918842  0.51275607]\n",
      " [-1.33693237  1.51575198]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-1.33693237  1.68291796]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-1.33693237  1.18142001]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [-1.33693237  2.3515819 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [-1.81580434  2.01724993]\n",
      " [ 0.57855553  0.34559009]\n",
      " [-0.37918842  1.01425402]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.81580434  1.85008394]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  1.51575198]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 0.09968355  0.51275607]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.85806039  1.85008394]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-1.81580434  2.01724993]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-1.33693237  0.34559009]\n",
      " [-1.33693237  0.51275607]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [-1.81580434  1.51575198]\n",
      " [ 0.09968355  1.01425402]\n",
      " [-1.33693237  1.34858599]\n",
      " [-1.33693237  1.51575198]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.37918842 -0.65740582]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.09968355  1.01425402]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.09968355  0.34559009]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.37918842  0.34559009]\n",
      " [-2.29467632  1.68291796]\n",
      " [-0.85806039  0.01125812]\n",
      " [-0.37918842  0.84708804]\n",
      " [-1.33693237  0.51275607]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-0.37918842  0.34559009]\n",
      " [ 1.0574275  -0.8245718 ]\n",
      " [-0.37918842  0.01125812]\n",
      " [-1.33693237  1.51575198]\n",
      " [-1.81580434  1.51575198]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-1.33693237  1.85008394]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-1.81580434  2.3515819 ]\n",
      " [-1.81580434  0.51275607]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.37918842  0.67992205]\n",
      " [-0.85806039  0.34559009]\n",
      " [-0.37918842  1.51575198]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.09968355  0.1784241 ]\n",
      " [ 0.57855553 -0.49023984]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.85806039  1.34858599]\n",
      " [-0.37918842 -0.32307385]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.09968355  0.34559009]\n",
      " [-0.85806039  1.18142001]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.09968355 -0.8245718 ]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [-0.37918842  0.84708804]\n",
      " [-0.85806039  1.18142001]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [-1.81580434  1.34858599]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.85806039  0.51275607]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-1.33693237  1.68291796]\n",
      " [-2.29467632  1.51575198]\n",
      " [-1.33693237  0.34559009]\n",
      " [-0.37918842  0.01125812]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-1.81580434  2.18441591]\n",
      " [-2.29467632  1.18142001]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  0.84708804]\n",
      " [-0.37918842 -0.49023984]\n",
      " [-1.33693237  2.01724993]\n",
      " [-1.33693237  1.34858599]\n",
      " [-2.29467632  1.85008394]\n",
      " [-1.81580434  1.51575198]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 0.09968355 -0.8245718 ]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.85806039  0.34559009]\n",
      " [-0.37918842  0.51275607]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [-0.37918842 -1.82756771]\n",
      " [-0.37918842  1.34858599]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355 -1.49323574]\n",
      " [ 2.01517145  0.34559009]\n",
      " [-0.37918842  0.34559009]\n",
      " [-0.37918842  0.84708804]\n",
      " [-2.29467632  1.18142001]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 0.57855553 -0.49023984]\n",
      " [ 0.09968355 -1.66040172]\n",
      " [-0.37918842  0.34559009]\n",
      " [-0.37918842  1.18142001]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.37918842 -0.15590787]\n",
      " [-0.85806039  1.51575198]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.81580434  0.67992205]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [-2.29467632  1.34858599]\n",
      " [-1.81580434  1.68291796]\n",
      " [-1.33693237  1.01425402]\n",
      " [-0.85806039  0.67992205]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [-0.85806039  2.01724993]\n",
      " [-1.81580434  1.68291796]\n",
      " [-1.33693237  1.68291796]\n",
      " [-2.29467632  2.68591386]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553  0.34559009]\n",
      " [-0.85806039  0.1784241 ]\n",
      " [ 0.09968355  0.01125812]\n",
      " [-0.85806039  0.67992205]\n",
      " [-0.37918842  1.01425402]\n",
      " [-0.37918842  0.34559009]\n",
      " [ 0.09968355  0.84708804]\n",
      " [-2.77354829  0.34559009]\n",
      " [-0.85806039  0.84708804]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [-0.37918842 -0.49023984]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [-0.85806039  0.34559009]\n",
      " [-0.37918842  0.34559009]\n",
      " [-1.33693237  0.1784241 ]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [-1.33693237  1.51575198]\n",
      " [-0.85806039  0.51275607]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 0.57855553 -1.15890377]\n",
      " [-1.33693237  1.85008394]\n",
      " [-0.37918842 -0.65740582]\n",
      " [-0.37918842  0.67992205]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 0.09968355 -1.32606976]\n",
      " [-0.37918842  0.34559009]\n",
      " [-0.85806039  1.68291796]\n",
      " [ 1.53629948  0.01125812]\n",
      " [-0.85806039  1.34858599]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [-0.85806039 -0.15590787]\n",
      " [-1.33693237  1.01425402]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [ 1.53629948 -0.15590787]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-0.85806039  0.01125812]\n",
      " [-0.37918842  1.01425402]\n",
      " [ 0.57855553  0.1784241 ]\n",
      " [ 1.0574275  -0.65740582]\n",
      " [-1.81580434  1.34858599]\n",
      " [ 0.09968355  1.01425402]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 1.53629948  0.01125812]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.65740582]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [ 1.0574275  -0.32307385]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 1.0574275  -0.49023984]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.33693237  1.68291796]\n",
      " [-0.85806039  0.84708804]\n",
      " [-0.37918842  0.1784241 ]\n",
      " [ 1.0574275   0.34559009]\n",
      " [ 0.57855553 -0.8245718 ]\n",
      " [-1.33693237  1.34858599]\n",
      " [ 0.09968355  0.01125812]\n",
      " [ 0.09968355 -1.15890377]\n",
      " [ 0.57855553 -0.99173779]\n",
      " [-0.85806039  1.01425402]\n",
      " [ 1.0574275  -0.49023984]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(xtest_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0543 - binary_accuracy: 0.9867\n",
      "[0.05429443344473839, 0.9866666793823242]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 2.0298 - binary_accuracy: 0.5133\n",
      "[2.029787063598633, 0.5133333206176758]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "xtest_cp = scaler.transform(xtest_cp)\n",
    "\n",
    "y_pred_cp = model.predict(xtest_cp)\n",
    "\n",
    "score = model.evaluate(xtest_cp, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict class for text example\n",
    "\n",
    "predicted_classes = np.where(y_pred > 0.6, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       154\n",
      "           1       0.97      1.00      0.99       146\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "\n",
    "print(classification_report(y_test, predicted_classes, labels=[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Very important note:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* when using binary crossentropy use the following to get the final predictions: \n",
    "\n",
    "`\n",
    "predicted_classes = np.where(y_pred > 0.5, 1,0)\n",
    "`\n",
    "\n",
    "* when using sparse categorical crossentropy use the following to get the final predictions: \n",
    "\n",
    "`\n",
    "predicted_classes = np.argmax(y_pred, axis=-1)\n",
    "`\n",
    "\n",
    "**Why?**\n",
    "\n",
    "* the outputs we get from our neural network are probabilities that some value belongs in some class\n",
    "    * we use the code above to turn those probabilities into actual class predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Take-home exercise`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Using the dataset available at `https://edlitera-datasets.s3.amazonaws.com/breast_cancer_data.csv`, create a model that can predict whether a person has a benign tumor or a malignant tumor. The`diagnosis` column indicates whether a person has a bening tumor (0) or a malignant tumor (1). Try to achieve the best possible accuracy you can by modifying hyperparameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Solution:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Saving and loading models in Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* saving a model in Keras can be done in three ways, depending on the need\n",
    "    * **saving the whole model**\n",
    "    * **saving only the arhitecture (configuration) of the model**\n",
    "    * **saving only the model weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Model arhitecture**\n",
    "\n",
    "* specifies the model layers and how they are connected\n",
    "\n",
    "* allows us to create a variant of the model that can be freshly initialized and that has no compilation information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Model weights**\n",
    "\n",
    "* the part of the model that is optimized\n",
    "\n",
    "\n",
    "* we can apply the weights we save to some other untrained model that is of the same structure as our original one without needing to train from the beginning\n",
    "\n",
    "\n",
    "* **ESSENTIAL FOR TRANSFER LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Model compilation information**\n",
    "\n",
    "* the arguments we set for the **`compile() method`** when creating and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Model optimizer state**\n",
    "\n",
    "* very important in some cases\n",
    "\n",
    "* if we want to completely recreate some conditions, optimizer state is extremely important\n",
    "    * a lot of popular optimizers have values that change during training, so starting with a fresh optimizer would not lead to good results even if we used the same model configuration and the same weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Saving and loading the whole model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we can save a whole model to a single artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* this saves the following:\n",
    "\n",
    "    * **model architecture**\n",
    "    * **model weights** \n",
    "    * **model compilation information**\n",
    "    * **model optimizer state**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Saving a model**\n",
    "\n",
    "* done using the following code\n",
    "```\n",
    "model.save('path/to/location')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Loading a model**\n",
    "\n",
    "* done using the following code\n",
    "```\n",
    "model.load_model('path/to/location')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Save model\n",
    "\n",
    "model.save(\"my_model.h5\")\n",
    "\n",
    "\n",
    "# Load saved model\n",
    "\n",
    "recreate_model = load_model(\"my_model.h5\")\n",
    "recreate_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Saving and loading the architecture of a model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we can save the architecture of a single layer, of a whole sequential model or the arhitecture of a whole functional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Saving the architecure of a single layer and loading it**\n",
    "\n",
    "```\n",
    "\n",
    "layer = keras.layers.Dense(20, activation=\"relu\", input_shape=(10,)))\n",
    "\n",
    "layer_config = layer.get_config()\n",
    "\n",
    "new_layer = keras.layers.Dense.from_config(layer_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Saving the architecure of a Sequential Model and loading it**\n",
    "\n",
    "\n",
    "```\n",
    "model = keras.Sequential([keras.Input((32,)), keras.layers.Dense(1)])\n",
    "\n",
    "model_config = model.get_config()\n",
    "\n",
    "new_model = keras.Sequential.from_config(model_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Saving the architecure of a Functional Model and loading it**\n",
    "\n",
    "\n",
    "```\n",
    "x = keras.Input((50,))\n",
    "out = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(x, out)\n",
    "\n",
    "model_config = model.get_config()\n",
    "\n",
    "\n",
    "new_model = keras.Model.from_config(model_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Saving and loading the weights of a model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* useful when we just need to perform inference using a model\n",
    "    * the optimizer state and the compilation information are then useless, so we can load only weights and apply them to some model architecture they fit\n",
    "    \n",
    "    \n",
    "* makes it easy to perform **transfer learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Saving weights**\n",
    "\n",
    "```\n",
    "model.save_weights(\"pretrained_checkpoint\")\n",
    "```\n",
    "\n",
    "**Loading weights**\n",
    "\n",
    "```\n",
    "model.load_weights(\"pretrained_checkpoint\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `How to further improve models using regularization techniques`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we test how our models perform on some type of testing dataset\n",
    "    * it gives us an idea of how good our model generalizes on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* achieving the proper fit is very hard\n",
    "    * deep learning models are prone to overfitting more than underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **getting to the global minima is not necessary**\n",
    "    * well designed and regularized complex Deep Learning models can converge into local minima that are very, very close to the global minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regularization methods:**\n",
    "\n",
    "   * `Early Stopping`\n",
    "   * `L2 Regularization`\n",
    "   * `L1 Regularization`\n",
    "   * `Dropout`\n",
    "   * `Batch Normalization`\n",
    "\n",
    "    \n",
    "**Honourable mentions:**\n",
    "\n",
    "* `Multitask Learning`\n",
    "* `Parameter Sharing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we typically use more than one regularization method when designing neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Early stopping`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* very common way of avoiding overfitting\n",
    "    * used because it is very easy to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* compares the error we get on the training set to the error we get on the validation dataset\n",
    "    * if the difference between these two errors is big, it means our model has started overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* using early stopping we can select that state of the model where the validation error was smallest\n",
    "    * essentially, we select those weights that were present when our model performed best on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://edlitera-images.s3.amazonaws.com/early_stopping.png\" width=\"700\">\n",
    "\n",
    "source:\n",
    "<br>\n",
    "https://www.semanticscholar.org/paper/Deep-Learning-for-NLP-and-Speech-Recognition-Kamath-Liu/6fe17c5de719df4dcbdb7967e14f7b457ec8c2ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Dropout`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* a bit like an ensemble technique for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* randomly \"drops out\" (removes) connections in the neural network\n",
    "    * this makes it impossible for any prediction to completely depend on a single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Batch Normalization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* created to combat **internal covariate shift**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Internal covariate shift**\n",
    "\n",
    "* we usually normalize data before feeding it into networks (or any other ML model)\n",
    "\n",
    "* problem with deep learning: distribution changes frequently during the learning procedure\n",
    "    * outputs of layers become non-normalized inputs for the subsequent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Batch normalization = normalizing the outputs of intermediate layers during training** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**How it works:**\n",
    "\n",
    "* the output of a hidden layer gets normalized using the mean and variance of the mini-batch we are currently training on before being fed as an input into the next layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Benefit:**\n",
    "\n",
    "* faster learning - learning rate can be higher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Important:**\n",
    "\n",
    "* Batch Normalization captures the **moving average of the mean and variance**\n",
    "    * this allows it to fix them at inference time to make sure that they don't affect predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
