{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Transfer Learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* taking features learned on one problem and using them to solve some other similar problem\n",
    "    * e.g. using the features from a model that was trained to identify bicycles to train a model that can identify motorcycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* inspired by how humans learn\n",
    "    * when we learn a new skill we use old knowledge to help us learn faster\n",
    "    * e.g. if you want to learn how to drive a stick shift car you can use the experience you gained driving an automatic transmission car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Why use transfer learning:**\n",
    "\n",
    "* there is an almost limitless amount of data available today\n",
    "   * BUT: raw unstructured data doesn't help us train supervised learning algorithms\n",
    "\n",
    "\n",
    "* to train algorithms we need clean, properly labeled data\n",
    "    * `Deep Learning` models require A LOT of data for training\n",
    "    \n",
    "\n",
    "* by leveraging what other models already learned we can reduce the amount of data we need to train a `Deep Learning` model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  `Transfer Learning in Deep Learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Deep Learning models are extremely well suited to inductive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Two most popular strategies\n",
    "<br>\n",
    "    \n",
    "\n",
    "**1.** `Using pre-trained models as feature extractors`\n",
    "\n",
    "\n",
    "**2.** `Fine tuning pre-trained models`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Using pre-trained models as feature extractors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* each layer of Deep Learning models arhitecture learns different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* perform transfer learning by using a trained model and 'freezing' certain layers of it\n",
    "    * this acts as feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The procedure**\n",
    "\n",
    "* we find a model that solves a problem similar to the one we have\n",
    "    \n",
    "* we \"cut\" out some of the layers\n",
    "\n",
    "* we connect the remaining layers of the original model to our own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Simplified:**\n",
    "\n",
    "* remove the last layer from some already trained network\n",
    "\n",
    "* add what we want to the end of that network\n",
    "  * can be a classifier, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://edlitera-images.s3.amazonaws.com/transfer_learning_extract_features.png\" width=\"1500\"/>\n",
    "\n",
    "source:\n",
    "<br>\n",
    "https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* an extension to the RNN architecture\n",
    "    * compensates for the shortcomings of the RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* arhitecture that revolutionized how we solve sequence-to-sequence tasks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* has mostly replaced LSTMs in a lot of tasks today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Main reasons:**\n",
    "\n",
    "* today GPUs and TPUs are used for Deep Learning\n",
    "    * most neural networks benefit from paralel processing\n",
    "    \n",
    "* problem with RNNs: \n",
    "    * data needs to be fed into the network sequentially\n",
    "    * this stops us from utilizing the real power of GPUs and TPUs\n",
    "    * therefore, RNNs take longer to train \n",
    "\n",
    "* transformers don't have this problem\n",
    "    * they also model dependencies better\n",
    "    * they achieve better results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Transformers structure`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"https://edlitera-images.s3.amazonaws.com/transformers_structure.png\" width=\"600\" >\n",
    "\n",
    "source:\n",
    "<br>\n",
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* easy to notice a few new concepts (positional encoding, attention, there is also a special type of normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the structure is fairly complicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* explaining them in detail is beyond the scope of this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"https://edlitera-images.s3.amazonaws.com/transformers_structure_parts.png\" width=\"600\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Popular transformers arhitectures`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* nowadays there are many transformers arhitectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* some are widely know, some are more obscure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the two most famous examples are **BERT** and **GPT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* they are essentially variants of the transformer network where only one half is used\n",
    "    * **BERT** - arhitecture created by **stacking the encoder part** of the transformers network\n",
    "    * **GPT** - arhitecture created by **stacking the decoder part** of the transformers network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://edlitera-images.s3.amazonaws.com/BERT_and_GPT.png\" width=\"1400\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* for us, BERT (and its variants) will better serve our purpose so let's explain how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `BERT - Bidirectional Encoder Representation from Transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* created from the encoder part of the transformers network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* trains in two phases:\n",
    "    <br>\n",
    "    \n",
    "    * **pre-training** - model learns about language and context \n",
    "    * **fine tuning** - model learns how to solve a specific problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `BERT pretraining`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* learns the concepts of language and context by training (in an unsupervised way) on two problems at the same time:\n",
    "\n",
    "   <br>\n",
    "   \n",
    "   * **Masked Language Modeling** \n",
    "   \n",
    "   * **Next Sentence Prediction** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Masked Language Modeling**\n",
    "\n",
    "<br>\n",
    "\n",
    "* the model is given sentences with certain words hidden (\"masked\") and needs to learn to predict those masked words\n",
    "    * layman's terms: fill in the blanks in the sentence problem\n",
    "\n",
    "\n",
    "* helps BERT learn how bidirectional context works in sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Next Sentence Prediction**\n",
    "\n",
    "* similar to a binary classification problem\n",
    "\n",
    "* the model is given two sentences: sentence A and sentence B\n",
    "\n",
    "* it needs to predict whether sentence B follows sentence A\n",
    "\n",
    "* helps BERT understand the concept of context over multiple sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `BERT fine tuning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* prime example of transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* after pretraining we can fine tune BERT to solve various NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **very fast: most of the model knowledge comes from what it learned during pretraining**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How it works**\n",
    "\n",
    "* we simply add output layers to the end of the network\n",
    "\n",
    "\n",
    "* depending on the layers we add, we can fine tune BERT to solve different tasks\n",
    "\n",
    "\n",
    "* **the majority of the network stays the same (the pretrained part), only the last few layers change depending on what task we want to solve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Creating transformers models with Hugging Face and ktrain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* there are a lot of different models that are used for text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* new neural network arhitectures get created very often, however most of them are not publicly available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* it is common practice to use pre-trained models and modify them for their own purposes \n",
    "    * unless you can do in-house research and development in the field of Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **models based on transformers tipically perform best for the purposes of text classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* the easiest way to work with transformers i.e. to use pretrained transformers models for your own purposes is to use the **`HuggingFace transformers library`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* to simplify using HuggingFace transformers, a Python library called **`ktrain`** was created\n",
    "    * lightweight wrapper for `Tensorflow` and `Keras`\n",
    "    * you can build standard Deep Learning models with it\n",
    "    * it can easily interface with the `HuggingFace` transformers package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Hugging Face`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* an extremely popular Python transformers package, available for both Pytorch and Tensorflow (and Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tons of community support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* provides easy way to work with many different transformers arhitectures such as:\n",
    "    <br>\n",
    "    \n",
    "    * BERT\n",
    "    * RoBERTa\n",
    "    * DistilBERT\n",
    "    * GPT\n",
    "    * GPT-2\n",
    "    * XLNet\n",
    "    * T5\n",
    "    * etc.\n",
    "    \n",
    "\n",
    "**These are just some of the pre-trained models that are available, there are currently over 10,000 pretrained models available on Hugging Face's model page: https://huggingface.co/models?p=0**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* that doesn't mean there are over 10,000 different pre-trained arhitectures\n",
    "\n",
    "* the package contains a finite amount of transformers arhitectures (essentially every popular transformers arhitecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**There is also support for training models using Amazon SageMaker !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `ktrain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* a Python library inspired by networks such as `fastai` and `ludwig`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* designed to make creating complex models as accessible as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* great for transfer learning purposes\n",
    "    <br>\n",
    "    \n",
    "    * supports a lot of pretrained models that work with text data, images, graph data and even tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Why is ktrain important to us?**\n",
    "\n",
    "* allows us to fine tune transformers extremely easy\n",
    "\n",
    "\n",
    "* we can choose one of the many models offered by HuggingFace \n",
    "    * this means that we can always find a pretrained transformers model that will help us solve our task\n",
    "\n",
    "\n",
    "* solves one of the biggest problems of transformers for beginners: coding them\n",
    "    * transformers are incredibly complex to code and tune properly\n",
    "    * without libraries such as `ktrain`, implementing and using them is nearly impossible for beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Why not use `ktrain` for everything?**\n",
    "\n",
    "* it turns a neural network model into even more of a black box model\n",
    "\n",
    "\n",
    "* offering complex functionality with just a few lines of code is great, BUT...\n",
    "    * we are a lot more limited in what we can actually modify\n",
    "    * customization is hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "* use **`ktrain`** to fine tune transformers models\n",
    "    * try to find a model on HuggingFace that will best suit your needs\n",
    "        * e.g. a model which was trained on data that is as similar as possible to yours\n",
    "    * or just use one of the generally good models\n",
    "        * e.g. BERT or its variations\n",
    "   \n",
    "   \n",
    "* **do not always default to transformers**\n",
    "    * training transformers, even just fine tuning, takes a lot more time than training simpler RNN based networks\n",
    "    * try training an RNN based network first\n",
    "    * if results are bad, try using transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Also keep in mind:**\n",
    "\n",
    "\n",
    "* if you have computational resources available, fine tune a transformers model using ktrain _before doing anything else_\n",
    "\n",
    "\n",
    "* if you get extremely bad results with transformers, your data likely isn't very good for solving your problem\n",
    "\n",
    "    * things to try in this case: add some preprocessing steps, remove other etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Training a transformer model with HuggingFace and ktrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load in our data and create a Dataframe\n",
    "\n",
    "df = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/imdb_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first five rows\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define independent feature\n",
    "\n",
    "X = df[\"review\"]\n",
    "\n",
    "# Define dependent feature\n",
    "\n",
    "y = df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Separate data into training data and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Separate data into training data and validation data\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choose one of the available models from the HuggingFace website\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the transformer model\n",
    "\n",
    "transformer = text.Transformer(MODEL_NAME, maxlen=100, class_names=[\"negative\", \"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing train...\n",
      "language: en\n",
      "train sequence lengths:\n",
      "\tmean : 231\n",
      "\t95percentile : 591\n",
      "\t99percentile : 908\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? False\n",
      "preprocessing test...\n",
      "language: en\n",
      "test sequence lengths:\n",
      "\tmean : 230\n",
      "\t95percentile : 577\n",
      "\t99percentile : 917\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess train and validation data\n",
    "\n",
    "train = transformer.preprocess_train(X_train.values, y_train.values)\n",
    "valid = transformer.preprocess_test(X_valid.values, y_valid.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define model task\n",
    "\n",
    "model = transformer.get_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define model \n",
    "\n",
    "learner = ktrain.get_learner(model, train_data=train, val_data=valid, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using onecycle policy with max lr of 2e-05...\n",
      "2000/2000 [==============================] - 441s 208ms/step - loss: 0.3907 - accuracy: 0.8183 - val_loss: 0.3107 - val_accuracy: 0.8636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21eb7966460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train for one epoch\n",
    "# Set learning rate as 2e-5\n",
    "# Train for one epoch\n",
    "\n",
    "learner.fit_onecycle(2e-5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictor\n",
    "\n",
    "predictor = ktrain.get_predictor(learner.model, preproc=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Save predictor\n",
    "\n",
    "predictor.save(\"transformers_models/Distilbert_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load predictor\n",
    "\n",
    "predictor = ktrain.load_predictor(\"transformers_models/Distilbert_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initially I was put off renting this movie due to the jacket art for the DVD. In fact, this held true with friends of mine who didn\\'t rent it due to the art and the mental image(s) it conjured of being a movie that held little or no interest to me (or to my friends). But, I rented and watched it and was truly amazed.<br /><br />I agree with another user\\'s comments that this movie is not for everyone due to the blatant sexual inferences, so it is definitely not something I\\'d want young children to watch (and doubt seriously if they would understand it anyway).<br /><br />I enjoy movies like this whereby the character\\'s personalities and who they are are genuinely defined in a no-nonsense, direct way with no teasers to indicate they will turn out bad. The acting done ... was it acting? Ricci and Jackson performed so well, I was drawn into this movie not even realizing they were acting. Same thing with the story ... may seem far-fetched somewhat, but it was done so very, very well. It reminded me of another movie with Mel Gibson, Tim, where each character had limitations, whether mental or circumstantial, so were well-defined.<br /><br />I found much depth in this movie with the character\\'s involved, so feel that everyone involved (from the cameramen to the actors) should be commended on a perfect fit/result. After viewing this movie, I had talked to a couple of friends who had a negative approach to watching it like I did, so after hearing my comments, they rented and watched it. They, too, were quite surprised at how good it was. It is too bad that the art on the jacket was done the way it was since it is a turn off. I can see now how the art applies, but I\\'d not heard of this movie before, and the art was my first impression ... art sells or destroys DVD sales/rentals.<br /><br />These characters had more depth to them and good timing was allotted to give an audience like me time to absorb the \"feel\" for each. I felt I could trust the movie to flow well, and it did. So, with the jacket art aside, I would recommend watching this movie.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at an example\n",
    "\n",
    "X_test.tolist()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction for an example\n",
    "\n",
    "predictor.predict(X_test.tolist()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions for a small sample\n",
    "\n",
    "y_pred = predictor.predict(X_test[:100].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert predictions from strings to integers\n",
    "# \"positive\" gets converted into 1, \"negative\" into 0\n",
    "\n",
    "true_predictions = []\n",
    "\n",
    "for prediction in y_pred:\n",
    "    if prediction == \"positive\":\n",
    "        true_predictions.append(1)\n",
    "    else:\n",
    "        true_predictions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89        47\n",
      "           1       0.89      0.92      0.91        53\n",
      "\n",
      "    accuracy                           0.90       100\n",
      "   macro avg       0.90      0.90      0.90       100\n",
      "weighted avg       0.90      0.90      0.90       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report for all examples\n",
    "\n",
    "#print(classification_report(y_test, true_predictions))\n",
    "\n",
    "#Print classification report for the small sample\n",
    "\n",
    "y_test_sample = y_test[:100]\n",
    "\n",
    "print(classification_report(y_test_sample, true_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
