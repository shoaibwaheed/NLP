{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed730457",
   "metadata": {},
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eab38c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d9b52",
   "metadata": {},
   "source": [
    "# Take-home exercises Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa4998",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e12b23",
   "metadata": {},
   "source": [
    "\n",
    "# First notebook - Introduction to Deep Learning model arhitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835db2a",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce2ae9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Take-home exercise`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360323b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Using both the sequential model API and the functional model API, create a model that would work with a tabular dataset that contains 65 columns. 64 of them represent independent features, and one represents dependent features. Use the ReLU activation function for the hidden layers. If the task is to perform binary classification use the appropriate activation function for the output layer, and the appropriate number of neurons in the output layer.**\n",
    "\n",
    "**Note:** when determining the number of layers and neurons try to follow the general rules of thumb that we mentioned before\n",
    "\n",
    "**For the model created using the Sequential API print out a model summary. Plot the model created using the Functional API.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f045c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0a1e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e2375",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model arhitecture using the Sequential API\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(64,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e22835",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model arhitecture using the Functional API\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "# Define input layer\n",
    "\n",
    "input_layer_1 = Input(shape=(64,))\n",
    "\n",
    "# Define hidden layers\n",
    "\n",
    "hidden_layer_1 = Dense(32, activation='relu')(input_layer_1)\n",
    "hidden_layer_2 = Dense(16, activation='relu')(hidden_layer_1)\n",
    "\n",
    "# Define output layer\n",
    "\n",
    "prediction_layer = Dense(1, activation='sigmoid')(hidden_layer_2)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_layer_1, outputs=prediction_layer)\n",
    "\n",
    "\n",
    "# Bonus: add the argument rankdir with the value 'LR' to ask for a horizontal plot (my personal preference)\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a4872",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f34d8",
   "metadata": {},
   "source": [
    "# Second notebook - Training Deep Learning models with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848a668c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ef54a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Take-home exercise`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883bfd3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Using the dataset available at `https://edlitera-datasets.s3.amazonaws.com/breast_cancer_data.csv`, create a model that can predict whether a person has a benign tumor or a malignant tumor. The`diagnosis` column indicates whether a person has a bening tumor (0) or a malignant tumor (1). Try to achieve the best possible accuracy you can by modifying hyperparameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96dd244",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d21cc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7ca52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import BinaryAccuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb8a32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "df = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/breast_cancer_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11289bc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215597b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e159c4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Separate features from the label\n",
    "\n",
    "X = df.iloc[:, 1:]\n",
    "\n",
    "\n",
    "# Flatten data\n",
    "\n",
    "y = df[\"diagnosis\"]\n",
    "y = y.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc795d05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split train data into train and validation data\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524e479",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define scaler and scale data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d979c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define input dimension\n",
    "\n",
    "input_dimension = X_train.shape[1]\n",
    "\n",
    "print(input_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37640643",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(8, activation = 'relu', input_shape=(input_dimension, )))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadb86b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "\n",
    "optim = Adam()\n",
    "\n",
    "\n",
    "# Define loss\n",
    "\n",
    "loss_function = BinaryCrossentropy()\n",
    "\n",
    "\n",
    "# Define metric we will track \n",
    "\n",
    "metric = BinaryAccuracy()\n",
    "\n",
    "\n",
    "# Compile model\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optim,\n",
    "              metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f1626",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=150, \n",
    "    batch_size=64, \n",
    "    validation_data=(X_valid, y_valid), \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7134a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b3399",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Predict class for text example\n",
    "\n",
    "predicted_classes = np.where(y_pred > 0.6, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551f0b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "\n",
    "print(classification_report(y_test, predicted_classes, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ba8fc",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83462163",
   "metadata": {},
   "source": [
    "# Third notebook - Word embeddings and the Keras Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddd6c9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96272fbd",
   "metadata": {},
   "source": [
    "## No exercises assigned! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883d5e7",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1668bf",
   "metadata": {},
   "source": [
    "# Fourth notebook - Gensim and SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58b45c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be727826",
   "metadata": {},
   "source": [
    "### `Take-home exercise 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84615b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Create a `Skip-gram model` using `Gensim` and train it on the data inside the `Review Text` column of the dataset stored inside `https://edlitera-datasets.s3.amazonaws.com/womens_clothing_reviews.csv` file. After training take a look at what are the most similar words to the word `\"shirt\"` using that model.**\n",
    "\n",
    "**Note: train the model both with, and without preprocessing your text data (removing stop words etc.) and compare the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c68b6f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be64bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432dba74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17937743",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load in our data and create a Dataframe\n",
    "\n",
    "df = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/womens_clothing_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1e0e0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the first five rows of our Dataframe\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80e270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create function for preprocessing text data\n",
    "\n",
    "def preprocess_reviews(text, stop_words=None, stemmer=None):\n",
    "    text_data = text.lower() # lowercase text data\n",
    "    tokens = tokenizer.tokenize(text_data) # tokenize data using the given tokenizer\n",
    "    if stop_words is not None:\n",
    "        tokens = [t for t in tokens if not t in stop_words] # remove stop words\n",
    "    if stemmer is not None:\n",
    "        tokens = [stemmer.stem(t) for t in tokens] # stem text data\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693370b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d782f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define stemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece2c49",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d7fad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess review titles\n",
    "\n",
    "df[\"tokenized_reviews_with_preprocessing\"] = df[\"Review Text\"].apply(preprocess_reviews, \n",
    "                                                                     args=(stop_words, stemmer))\n",
    "\n",
    "\n",
    "df[\"tokenized_reviews_without_preprocessing\"] = df[\"Review Text\"].map(preprocess_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e532327",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc346ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create list of lists\n",
    "# every sublist contains a tokenized preprocessed sentence\n",
    "\n",
    "reviews_with_preprocessing = [row.split(\".\") for row in df[\"tokenized_reviews_with_preprocessing\"]]\n",
    "\n",
    "tokenized_reviews_with_preprocessing = [sublist[0].split() for sublist in reviews_with_preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d0245",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create list of lists\n",
    "# every sublist contains a tokenized sentence\n",
    "\n",
    "reviews_without_preprocessing = [row.split(\".\") for row in df[\"tokenized_reviews_without_preprocessing\"]]\n",
    "\n",
    "tokenized_reviews_without_preprocessing = [sublist[0].split() for sublist in reviews_without_preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5679c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train Skip-gram model\n",
    "\n",
    "skip_gram_with_preprocessing = Word2Vec(tokenized_reviews_with_preprocessing, min_count=3, size=300, window=3, sg=1)\n",
    "#skip_gram_with_preprocessing = Word2Vec(tokenized_reviews_with_preprocessing, min_count=3, vector_size=300, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f393fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train Skip-gram model\n",
    "\n",
    "skip_gram_without_preprocessing = Word2Vec(tokenized_reviews_without_preprocessing, min_count=3, size=300, window=3, sg=1)\n",
    "#skip_gram_without_preprocessing = Word2Vec(tokenized_reviews_without_preprocessing, min_count=3, vector_size=300, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6fe703",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at what are the most similar words to shirt \n",
    "\n",
    "print(skip_gram_with_preprocessing.wv.most_similar(\"shirt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c969f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at what are the most similar words to shirt \n",
    "\n",
    "print(skip_gram_without_preprocessing.wv.most_similar(\"shirt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd843178",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e05eed",
   "metadata": {},
   "source": [
    "### `Take-home exercise 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710ba2b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Load the dataset available at `https://edlitera-datasets.s3.amazonaws.com/womens_clothing_reviews.csv` and create a DataFrame from it. Using the \n",
    "data inside the `Review Text` column, try to predict the appropriate value for the `Recommended IND` column.**\n",
    "\n",
    "**Note 1: perform undersampling to balance out your dataset**\n",
    "\n",
    "**Note 2: since we haven't yet covered LSTMs you can use the model we used in class**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb1845",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e108dc4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0429a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get rid of randomness (as much as possible)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42) \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e9792",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load in our data and create a Dataframe\n",
    "\n",
    "df = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/womens_clothing_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d2e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9438e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Recommended IND\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c424d03",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Divide by class\n",
    "\n",
    "df_class_0 = df[df[\"Recommended IND\"] == 0]\n",
    "df_class_1 = df[df[\"Recommended IND\"] == 1]\n",
    "\n",
    "df_class_1 = df_class_1.sample(len(df_class_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b054b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_class_0, df_class_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb29fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cf06d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Separate dependent feature from the independent feature\n",
    "\n",
    "X = df[\"Review Text\"]\n",
    "\n",
    "y = df[\"Recommended IND\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1472b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Separate data into training data and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c5b8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Separate data into training data and validation data\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6d69e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "# Set number of words as 10 000\n",
    "# Set value of oov token\n",
    "# Leave everything else on default values\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=10_000, \n",
    "    oov_token = \"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052d4be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fit tokenizer on train data\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84390b45",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert into sequences of integers\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4a395",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define values important for padding\n",
    "\n",
    "max_length = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0382b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Pad train, validation and test data\n",
    "\n",
    "X_train = pad_sequences(X_train, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
    "X_valid = pad_sequences(X_valid, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
    "X_test = pad_sequences(X_test, padding=padding_type, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299472e0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define vocabulary size\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f26023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "\n",
    "embedding_vector = dict()\n",
    "\n",
    "word_embeddings = \"glove.6B.100d.txt\"\n",
    "\n",
    "f = open(word_embeddings, encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coef = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embedding_vector[word] = coef\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96501bd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an embedding matrix to use as weights for the embedding layer\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size,100))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06f310",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define embedding layer with pretrained weights\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, \n",
    "                            100, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=100, \n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e387d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(16, recurrent_dropout=0.2)) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a249b576",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=[BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676cecf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Train model\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2980f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06ea94",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create classification report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e136321",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d84825",
   "metadata": {},
   "source": [
    "### `Take-home exercise 3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1b36e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**In this exercise, you need to preprocess data stored in the `Review Text` column of the modified version of the `Womens clothing reviews` dataset. The dataset is available here: `https://edlitera-datasets.s3.amazonaws.com/womens_clothing_reviews.csv`**\n",
    "\n",
    "**Preprocessing steps:**\n",
    "\n",
    "* get rid of all the columns except the \"Review Text\" and \"Class Name\" columns\n",
    "* select the first 1000 rows of the DataFrame\n",
    "* remove punctuation\n",
    "* remove stop words\n",
    "* remove empty spaces\n",
    "* lemmatize text data \n",
    "\n",
    "**Do the preprocessing using `spaCy`. After you finish preprocessing the data in the `Review Text` column, create word vectors for the text using the `Word2Vec CBOW` model and `Gensim`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a2edd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75dbd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1e32b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load pipeline\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a60f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and create DataFrame\n",
    "\n",
    "df = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/womens_clothing_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8567df9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the first five rows\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2cbe47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Select the first 1000 rows\n",
    "\n",
    "df = df.iloc[:1000, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f82727",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Leave only the two columns\n",
    "\n",
    "df = df[[\"Review Text\", \"Class Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4243b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function that preprocesses data\n",
    "\n",
    "def preprocess_text(data):\n",
    "    document = nlp(data)\n",
    "    cleaned_data_generator = (token.lemma_ for token in document if not token.is_punct | token.is_space | token.is_stop)\n",
    "    cleaned_text = \" \".join(cleaned_data_generator).lower()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2622167",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "\n",
    "df[\"Review Text\"] = df[\"Review Text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17eac4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the first five rows\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffd2fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for CBOW\n",
    "\n",
    "reviews = [row.split(\",\") for row in df[\"Review Text\"]]\n",
    "\n",
    "tokenized_reviews = [sublist[0].split() for sublist in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93cd4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7695d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train CBOW model\n",
    "\n",
    "#CBOW_model = Word2Vec(tokenized_reviews, min_count=3, size=300, window=3, sg=0)\n",
    "CBOW_model = Word2Vec(tokenized_reviews, min_count=3, vector_size=300, window=3, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797f3a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Check similarity between \"small\" and \"petite\" using CBOW\n",
    "\n",
    "CBOW_model.wv.similarity(\"small\", \"petite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec7d38f",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c6926",
   "metadata": {},
   "source": [
    "# Fifth notebook - RNNs and LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109ad4c",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54436625",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LSTM take home exercise`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c9e89",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Train an `LSTM model` to classify wines into good wines and superior wines (classes 0 and 1), using the dataset stored in the `https://edlitera-datasets.s3.amazonaws.com/wine_data_classification.csv` file. Do not clean your data in any way. Instead, use it as is to prove that the model will outperform the classic Machine Learning models we trained earlier, even without text data preprocessing!**\n",
    "\n",
    "**When done training the model, print the classification report.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24869b4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94643b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae6bbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import data and create a DataFrame\n",
    "# Take a look at the first 5 rows\n",
    "\n",
    "wine_data = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/wine_data_classification.csv\")\n",
    "\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab6894",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode data\n",
    "\n",
    "wine_data[\"wine_type\"] = wine_data.wine_type.map({\"great_wine\": 0, \"superior_wine\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43599c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Undersample, to get matching number of samples for both classes\n",
    "\n",
    "count_class_0, count_class_1 = wine_data.wine_type.value_counts()\n",
    "\n",
    "wine_data_class_0 = wine_data[wine_data['wine_type'] == 0]\n",
    "wine_data_class_1 = wine_data[wine_data['wine_type'] == 1]\n",
    "\n",
    "wine_data_class_0_under = wine_data_class_0.sample(count_class_1)\n",
    "wine_data = pd.concat([wine_data_class_0_under, wine_data_class_1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fad21",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wine_data[\"wine_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8eb6f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "\n",
    "wine_data = wine_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d86c45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define independent feature\n",
    "\n",
    "X_wine = wine_data[\"description\"]\n",
    "\n",
    "# Define dependent feature\n",
    "\n",
    "y_wine = wine_data[\"wine_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b864c00",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Separate data into training data and testing data\n",
    "\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46759e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Separate data into training data and validation data\n",
    "\n",
    "X_wine_train, X_wine_valid, y_wine_train, y_wine_valid = train_test_split(\n",
    "    X_wine_train, y_wine_train, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827eaa9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "\n",
    "wine_tokenizer = Tokenizer(num_words=20_000, oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603e56a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fit tokenizer on train data\n",
    "\n",
    "wine_tokenizer.fit_on_texts(X_wine_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e821720",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert into sequences of integers\n",
    "\n",
    "X_wine_train = wine_tokenizer.texts_to_sequences(X_wine_train)\n",
    "X_wine_valid = wine_tokenizer.texts_to_sequences(X_wine_valid)\n",
    "X_wine_test = wine_tokenizer.texts_to_sequences(X_wine_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800094b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define values important for padding\n",
    "\n",
    "max_length = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ff83a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Pad train, validation and test data\n",
    "\n",
    "X_wine_train_padded = pad_sequences(X_wine_train, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
    "X_wine_valid_padded = pad_sequences(X_wine_valid, padding=padding_type, maxlen=max_length, truncating=trunc_type)\n",
    "X_wine_test_padded = pad_sequences(X_wine_test, padding=padding_type, maxlen=max_length, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ba5a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define vocabulary size\n",
    "\n",
    "wine_vocab_size = len(wine_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542769c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "\n",
    "embedding_vector = dict()\n",
    "\n",
    "word_embeddings = \"glove.6B.100d.txt\"\n",
    "\n",
    "f = open(word_embeddings, encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coef = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embedding_vector[word] = coef\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5887b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create an embedding matrix to use as weights for the embedding layer\n",
    "\n",
    "embedding_matrix = np.zeros((wine_vocab_size, 100))\n",
    "\n",
    "for word, i in wine_tokenizer.word_index.items():\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    \n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf2505",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define embedding layer with pretrained weights\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    wine_vocab_size, \n",
    "    100, \n",
    "    weights=[embedding_matrix], \n",
    "    input_length=100, \n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80b736",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "embedding_dim = 100\n",
    "input_dim = wine_vocab_size\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(64, recurrent_dropout=0.2)) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfe827",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "loss_function = BinaryCrossentropy()\n",
    "\n",
    "metric = BinaryAccuracy()\n",
    "\n",
    "optim = Adam()\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optim, metrics=BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3500c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Train model\n",
    "\n",
    "history = model.fit(\n",
    "    X_wine_train_padded, \n",
    "    y_wine_train, \n",
    "    batch_size=batch_size, \n",
    "    epochs=num_epochs, \n",
    "    verbose=1, \n",
    "    validation_data=(X_wine_valid_padded, y_wine_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e947da3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions using model\n",
    "\n",
    "y_wine_pred = model.predict(X_wine_test_padded)\n",
    "\n",
    "y_wine_pred = y_wine_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc298d22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a classification report \n",
    "\n",
    "print(classification_report(y_wine_test, y_wine_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76858b6e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab191e31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d566147",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
