{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Boosting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **`ensemble method`** that combines several predictors with low accuracy (**`weak learners`**) into a model with high accuracy (**`strong learner`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **the unit predictor is usually a decision tree (but can be any algorithm)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* models are built **iteratively**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of `boosting` algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there are multiple different types of **`boosting`** algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important `boosting` algorithms:**\n",
    "  \n",
    "   * **`AdaBoost`**\n",
    "  <br/><br/>\n",
    "   * **`Gradient boosting (simple)`**\n",
    "    <br/><br/>\n",
    "   * **`XGBoost`**\n",
    "   <br/><br/>\n",
    "   * **`CatBoost`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`AdaBoost`** is an older technique, and the results we get using it tend to be worse than those we get using the other mentioned techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`CatBoost`** is a newer technique, but doesn't enjoy the same amount of support as XGBoost, so we will skip it for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How does ` boosting` work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* starts with one predictor (one **`Decision Tree`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* creates the next predictor by improving on the previous predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* each new predictor tries to improve upon the previous predictor and so on\n",
    " <br/><br/>\n",
    "    * essentially each new predictor 'learns' from past mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* keeps adding new predictors **sequentially** until:\n",
    "    <br/><br/>\n",
    "    * a perfect predictor is created **or** the specified number of estimators is created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Gradient Boosting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* achieves better results than basic boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the training process is a bit more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* let's go through it step by step\n",
    "    * math gets complicated, so we will avoid it as much as we possibly can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a `gradient boosting` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edlitera-images.s3.us-east-1.amazonaws.com/gradient_boost.png\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "\n",
    "* $y$ is the actual value\n",
    "* $\\hat{y}$ is the predicted value\n",
    "* $r = y - \\hat{y}$ is the residual\n",
    "* $f(r)$ is the loss function that each predictor tries to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the first predictor is trained **on the training data set** and it predicts the average for all observations\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    * can use a variety of models as the unit predictor, but we tipically use decision trees: **`Gradient Tree Boosting`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* next, the _residual errors_ are computed\n",
    "    * **for classification:**\n",
    "        * calculate the true probability distribution for the classes that need to be predicted\n",
    "        * calculate the predicted probability distribution for the classes that need to be predicted\n",
    "        * use the <a href='https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence'>Kullback-Leibler divergence</a> to see how different the two probability distributions are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* next, add another predictor (decision tree) that attempts to minimize the _residuals_ computed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the procedure gets repeated until the residuals become zero or the specified number of predictors are created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **to make a prediction, you need to sum up all the values / probabilities predicted by each predictor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edlitera-images.s3.us-east-1.amazonaws.com/gradient_boost_2.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is it called `gradient boosting`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with each predictor, we attempt to reduce the prediction errors\n",
    "    <br>\n",
    "    \n",
    "    * i.e. we have a **loss function** we aim to minimize\n",
    "    <br>\n",
    "    \n",
    "    * this is done in a manner similar to **gradient descent**\n",
    "    <br>\n",
    "    \n",
    "    * the **'gradient'** is the **partial derivative of the loss function**\n",
    "        <br>\n",
    "        \n",
    "        * allows the model to find the **\"direction\"** in which to change the parameters in order to minimize the **loss function** as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* it's **boosting** because each new predictor aims to reduce the errors made by the previous predictor\n",
    "    <br>\n",
    "    \n",
    "    * i.e. **each predictor 'boosts' the accuracy of the predecesors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of `gradient boosting` machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learning rate\n",
    "    <br>\n",
    "    \n",
    "    * the amount by which to 'shrink' (reduce) the contribution of additional predictors / features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* number of estimators (predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* minimum number of observations in each leaf node, in each decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* maximum depth of each decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of `gradient boosting` algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* achieve excellent accuracy (often the best out of all supervised models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* no data preprocessing necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* flexible (many hyperparameters can be tuned to adapt the model to every problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantages of `gradient boosting` algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* computationally expensive and relatively slow to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* hard to tune (a lot of different hyperparameters need to be tuned for the model to work optimally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* can be hard to interpret in comparison to other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In `Scikit-Learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **`GradientBoostingClassifier`:** https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we won't use it: XGBoost is an improved version of **`gradient boosting`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `XGBoost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* specific implementation of **`gradient boosting machines`** that uses more accurate approximations to find the best tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements of `XGBoost` over standard `gradient boosting machines`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  uses second partial derivatives, which give it more information about the **gradient**\n",
    "<br>\n",
    "\n",
    "   * improves **loss function optimization**\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  advanced regularization (such as **L1 and L2 regularization**)\n",
    "\n",
    "    <br>\n",
    "    \n",
    "   * **improves model generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **parallelization**\n",
    "    <br>\n",
    "    \n",
    "    * trees are **NOT** built in parallel\n",
    "    <br>\n",
    "    \n",
    "    * the building of each tree **IS** parallelized\n",
    "        \n",
    "        * building of tree branches happen in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* includes **hardware optimization**\n",
    "    * improved performance\n",
    "    * can handle data that is too larget to fit in memory (**`out-of-core learning`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tuning `XGBoost` models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **full hyper-parameter list: https://xgboost.readthedocs.io/en/stable/index.html**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`booster`** - the unit predictor to use\n",
    "    <br>\n",
    "    \n",
    "    * **`gbtree`** - gradient boosted tree (the default)\n",
    "    <br>\n",
    "    \n",
    "    * other options: **`gblinear`**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`nthread`** - number of threads to use when building tree branches\n",
    "    <br>\n",
    "    \n",
    "    * defaults to maximum available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`eta`** or **`learning_rate`**\n",
    "    <br>\n",
    "    \n",
    "    * a number between 0 and 1\n",
    "    <br>\n",
    "    \n",
    "    * prevents overfitting\n",
    "    <br>\n",
    "    \n",
    "    * lower values will lead to better models, but will take longer\n",
    "    <br>\n",
    "    \n",
    "    * higher values will lead to worse models, but will take less time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`max_depth`** - maximum depth of each **`Decision Tree`**\n",
    "    <br>\n",
    "    \n",
    "    * number greater than 0, defaults to 6\n",
    "    <br>\n",
    "    \n",
    "    * higher values create more complex models\n",
    "        * slower\n",
    "        * more prone to **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`n_estimators`** - number of predictors (**`Decision Trees`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`gamma`** - minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    " <br>\n",
    "    \n",
    "    * the larger gamma is, the more conservative the algorithm will be\n",
    "     <br>\n",
    "    \n",
    "    * number greater than or equal to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`subsample`** - what portion of training instances to sample at each boosting step\n",
    " <br>\n",
    "    \n",
    "    * default 1 (i.e. use all instances for each predictor)\n",
    "     <br>\n",
    "    \n",
    "    * value between 0 and 1\n",
    "     <br>\n",
    "    \n",
    "    * values lower than 1 can reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`colsample_bytree`** - what portion of features (columns) to use at each boosting step\n",
    " <br>\n",
    "    \n",
    "    * default 1 (i.e. use all features for each predictor)\n",
    "     <br>\n",
    "    \n",
    "    * value between 0 and 1\n",
    "     <br>\n",
    "    \n",
    "    * values lower than 1 can reduce overfitting, increase speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`colsample_bylevel`** - what portion of features (columns) to use at each boosting step\n",
    " <br>\n",
    "    \n",
    "    * default 1 (i.e. use all features that are available for a predictor)\n",
    "     <br>\n",
    "    \n",
    "    * value between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`alpha`** and **`lambda`** - regularization terms\n",
    " <br>\n",
    "    \n",
    "    * default 1\n",
    "     <br>\n",
    "    \n",
    "    * can be any value\n",
    "     <br>\n",
    "    \n",
    "    * increasing the value will make the model more conservative (reduce overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`predictor`** - allows you to use CPU or GPU\n",
    " <br>\n",
    "    \n",
    "    * **`cpu_predictor`** - multicore CPU predictor\n",
    "     <br>\n",
    "    \n",
    "    * **`gpu_predictor`** - use GPU for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **`objective`** - specify the learning task\n",
    "    <br>\n",
    "    \n",
    "    * default **`reg:linear` (`Linear Regression`)**\n",
    "    <br>\n",
    "    \n",
    "    * can also be:\n",
    "        <br>\n",
    "        \n",
    "        * **`reg:logistic`** - **`Logistic regression`**\n",
    "        <br>\n",
    "    \n",
    "        * **`binary:logistic`** - **`Logistic regression`** for binary classification, output probability\n",
    "        <br>\n",
    "    \n",
    "        * **`multi:softmax`** - multiclass classification\n",
    "        <br>\n",
    "    \n",
    "            * need to also set **`num_class`** - number of classes in your dataset\n",
    "            <br>\n",
    "    \n",
    "            * outputs the predicted class\n",
    "            <br>\n",
    "    \n",
    "        * **`multi:softprob`** - multiclass classification\n",
    "            * outputs the predicted probability of each data point belonging to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `XGBoost` example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for the purposes of demonstrating XGBoost we will use a modified version of the publicly available dataset called **\"Water Quality\"**\n",
    "\n",
    "* it can be found on Kaggle: https://www.kaggle.com/adityakadiwal/water-potability\n",
    "* we need to create a model that will predict whether some water sample is potable (safe to drink) or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our data and create a Dataframe\n",
    "\n",
    "df = pd.read_csv(\"https://edlitera-datasets.s3.amazonaws.com/water_safety.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.085378</td>\n",
       "      <td>204.890455</td>\n",
       "      <td>20791.318981</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.057858</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>334.564290</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.541732</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>334.564290</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.417441</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436524</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.986339</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
       "0  7.085378  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
       "1  3.716080  129.422921  18630.057858     6.635246  334.564290    592.885359   \n",
       "2  8.099124  224.236259  19909.541732     9.275884  334.564290    418.606213   \n",
       "3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
       "4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
       "\n",
       "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0       10.379783        86.990970   2.963135           0  \n",
       "1       15.180013        56.329076   4.500656           0  \n",
       "2       16.868637        66.420093   3.055934           0  \n",
       "3       18.436524       100.341674   4.628771           0  \n",
       "4       11.558279        31.997993   4.075075           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first five rows of our Dataframe\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select independent features\n",
    "\n",
    "X = df.drop(\"Potability\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dependent feature (target)\n",
    "\n",
    "y = df[\"Potability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.333003</td>\n",
       "      <td>189.447965</td>\n",
       "      <td>32358.538595</td>\n",
       "      <td>6.346378</td>\n",
       "      <td>341.346443</td>\n",
       "      <td>490.625291</td>\n",
       "      <td>12.537466</td>\n",
       "      <td>61.581378</td>\n",
       "      <td>2.642004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.455599</td>\n",
       "      <td>152.386735</td>\n",
       "      <td>37620.248469</td>\n",
       "      <td>5.783580</td>\n",
       "      <td>334.564290</td>\n",
       "      <td>343.417588</td>\n",
       "      <td>14.401654</td>\n",
       "      <td>41.109869</td>\n",
       "      <td>4.476349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.610673</td>\n",
       "      <td>193.661870</td>\n",
       "      <td>20511.620832</td>\n",
       "      <td>6.654332</td>\n",
       "      <td>332.566990</td>\n",
       "      <td>394.595406</td>\n",
       "      <td>14.319384</td>\n",
       "      <td>39.497264</td>\n",
       "      <td>5.404818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.170588</td>\n",
       "      <td>208.279181</td>\n",
       "      <td>12806.832327</td>\n",
       "      <td>5.489259</td>\n",
       "      <td>322.923998</td>\n",
       "      <td>472.028286</td>\n",
       "      <td>13.389549</td>\n",
       "      <td>28.566364</td>\n",
       "      <td>3.995857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.114665</td>\n",
       "      <td>236.493275</td>\n",
       "      <td>26631.212874</td>\n",
       "      <td>6.266782</td>\n",
       "      <td>275.090870</td>\n",
       "      <td>496.989792</td>\n",
       "      <td>18.588534</td>\n",
       "      <td>58.313345</td>\n",
       "      <td>2.654594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>5.970036</td>\n",
       "      <td>230.449329</td>\n",
       "      <td>16324.115196</td>\n",
       "      <td>5.293919</td>\n",
       "      <td>315.186121</td>\n",
       "      <td>327.786642</td>\n",
       "      <td>13.325987</td>\n",
       "      <td>39.554249</td>\n",
       "      <td>5.428674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>8.558389</td>\n",
       "      <td>198.888868</td>\n",
       "      <td>8535.402956</td>\n",
       "      <td>4.796666</td>\n",
       "      <td>321.403477</td>\n",
       "      <td>453.571163</td>\n",
       "      <td>14.351624</td>\n",
       "      <td>74.326204</td>\n",
       "      <td>3.946228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>6.571099</td>\n",
       "      <td>207.741025</td>\n",
       "      <td>23372.882654</td>\n",
       "      <td>6.280195</td>\n",
       "      <td>313.846972</td>\n",
       "      <td>497.902320</td>\n",
       "      <td>12.874805</td>\n",
       "      <td>75.416710</td>\n",
       "      <td>4.555949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>7.689358</td>\n",
       "      <td>221.356885</td>\n",
       "      <td>30253.851103</td>\n",
       "      <td>6.269309</td>\n",
       "      <td>320.478106</td>\n",
       "      <td>529.746529</td>\n",
       "      <td>17.973277</td>\n",
       "      <td>84.696366</td>\n",
       "      <td>4.978556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>7.283914</td>\n",
       "      <td>197.602125</td>\n",
       "      <td>23112.504435</td>\n",
       "      <td>9.502809</td>\n",
       "      <td>332.331985</td>\n",
       "      <td>449.361679</td>\n",
       "      <td>14.458591</td>\n",
       "      <td>70.000219</td>\n",
       "      <td>3.571863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3276 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
       "0     7.333003  189.447965  32358.538595     6.346378  341.346443   \n",
       "1     5.455599  152.386735  37620.248469     5.783580  334.564290   \n",
       "2     7.610673  193.661870  20511.620832     6.654332  332.566990   \n",
       "3     8.170588  208.279181  12806.832327     5.489259  322.923998   \n",
       "4     8.114665  236.493275  26631.212874     6.266782  275.090870   \n",
       "...        ...         ...           ...          ...         ...   \n",
       "3271  5.970036  230.449329  16324.115196     5.293919  315.186121   \n",
       "3272  8.558389  198.888868   8535.402956     4.796666  321.403477   \n",
       "3273  6.571099  207.741025  23372.882654     6.280195  313.846972   \n",
       "3274  7.689358  221.356885  30253.851103     6.269309  320.478106   \n",
       "3275  7.283914  197.602125  23112.504435     9.502809  332.331985   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  \n",
       "0       490.625291       12.537466        61.581378   2.642004  \n",
       "1       343.417588       14.401654        41.109869   4.476349  \n",
       "2       394.595406       14.319384        39.497264   5.404818  \n",
       "3       472.028286       13.389549        28.566364   3.995857  \n",
       "4       496.989792       18.588534        58.313345   2.654594  \n",
       "...            ...             ...              ...        ...  \n",
       "3271    327.786642       13.325987        39.554249   5.428674  \n",
       "3272    453.571163       14.351624        74.326204   3.946228  \n",
       "3273    497.902320       12.874805        75.416710   4.555949  \n",
       "3274    529.746529       17.973277        84.696366   4.978556  \n",
       "3275    449.361679       14.458591        70.000219   3.571863  \n",
       "\n",
       "[3276 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at our independent features\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3271    0\n",
       "3272    0\n",
       "3273    0\n",
       "3274    0\n",
       "3275    0\n",
       "Name: Potability, Length: 3276, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at our dependent feature\n",
    "\n",
    "y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into training data and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=41\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple XGBoost classifier\n",
    "\n",
    "classifier = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values from the test dataset\n",
    "# using the trained model\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7822990844354019\n"
     ]
    }
   ],
   "source": [
    "# Print model accuracy by comparing predictions with real values\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[526  82]\n",
      " [132 243]]\n"
     ]
    }
   ],
   "source": [
    "# As mentioned before, presicion by itself is not a good enough measure\n",
    "# for our model, so we will create a confusion matrix\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy Score: 0.7822990844354019')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Reds')\n",
    "plt.ylabel(\"True value\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "all_sample_title = f\"Accuracy Score: {accuracy_score}\"\n",
    "plt.title(all_sample_title, size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       608\n",
      "           1       0.75      0.65      0.69       375\n",
      "\n",
      "    accuracy                           0.78       983\n",
      "   macro avg       0.77      0.76      0.76       983\n",
      "weighted avg       0.78      0.78      0.78       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the classification report\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will try to find the best possible hyperparameters using a **grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters we want to check\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': range (2, 10, 1),\n",
    "    'n_estimators': range(60, 220, 40),\n",
    "    'learning_rate': [0.1, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid search \n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=10,\n",
    "    cv=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  30 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=10)]: Done 180 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=10)]: Done 430 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=10)]: Done 480 out of 480 | elapsed:   31.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster=None,\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                                     importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=0.300000012,\n",
       "                                     max_delta_step=0, max_depth=6,\n",
       "                                     min_child_weight=1, missing=nan,\n",
       "                                     monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=0,\n",
       "                                     num_parallel_tree=1, random_state=0,\n",
       "                                     reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, subsample=1,\n",
       "                                     tree_method=None,\n",
       "                                     validate_parameters=False,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=10,\n",
       "             param_grid={'learning_rate': [0.1, 0.01, 0.05],\n",
       "                         'max_depth': range(2, 10),\n",
       "                         'n_estimators': range(60, 220, 40)},\n",
       "             scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the grid search \n",
    "\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the optimized model\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the optimized classifier \n",
    "\n",
    "optimized_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Train the optimized classifier\n",
    "\n",
    "optimized_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values from the test dataset\n",
    "# using the trained model\n",
    "\n",
    "y_pred = optimized_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7772126144455748\n"
     ]
    }
   ],
   "source": [
    "# Print model accuracy by comparing predictions with real values\n",
    "\n",
    "accuracy_score = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83       608\n",
      "           1       0.75      0.62      0.68       375\n",
      "\n",
      "    accuracy                           0.78       983\n",
      "   macro avg       0.77      0.75      0.75       983\n",
      "weighted avg       0.77      0.78      0.77       983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the classification report\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `XGBoost` take-home exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset you will be working with in this exercise, available here `https://edlitera-datasets.s3.amazonaws.com/hotel_booking_data.csv`, is a modified version of the `Hotel Booking` dataset. It contains information about hotel guests and their stays.**\n",
    "\n",
    "<br>\n",
    "\n",
    "**Create an `XGBoost` model that will predict whether a potential guest is going to cancel their reservation or not (the dependent feature is `canceled`). You will need to use the available 24 independent features.**\n",
    "\n",
    "<br>\n",
    "\n",
    "**You will also need to calculate the `accuracy`, create a `confusion matrix` and create a `classification report` in order to evaluate your classification model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Solution` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
