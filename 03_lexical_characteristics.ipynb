{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711175b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7683487",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Lexical characteristics`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5067e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* process of segmenting text into lexical expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d659693",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* converting text into base word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb03e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Different types of representations:**\n",
    "   <br>\n",
    "   \n",
    "   * `word level representations`\n",
    "     <br>\n",
    "   \n",
    "   * `sentence level representations`\n",
    "     <br>\n",
    "   \n",
    "   * `document level representations`\n",
    "     <br>\n",
    "   \n",
    "   * `dense representations`\n",
    "     <br>\n",
    "   \n",
    "   * `embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4ad99",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**`Dense representations` and `embeddings` are more complex topics, so we will cover them later on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92ec22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Word level representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3aaba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* after a **`morpheme`**, a word is the most elementary part of some text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9dc20d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* often the first thing we do when processing text is separate it into words\n",
    "    * there are exceptions (e.g. when dealing with extremely noisy data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeec94b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Tokenization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7bcbc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* separating text into elements that hold some meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea704d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tokens are most often one of the following:\n",
    "  <br>\n",
    "  \n",
    "  * words\n",
    "   <br>\n",
    "  \n",
    "  * numbers\n",
    "   <br>\n",
    "  \n",
    "  * punctuation marks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957a392",
   "metadata": {},
   "source": [
    "* tokens can also be multiple words\n",
    "    <br>\n",
    "    \n",
    "    * e.g. \"the one and only\"\n",
    "    * it all depends on how we define what a token is when we tokenize our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790d7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example before tokenization:**\n",
    "\n",
    "\"The dog sits on the porch.\"\n",
    "\n",
    "**Example after tokenization:**\n",
    "\n",
    "\"The\" \n",
    "\n",
    "\"dog\"  \n",
    "\n",
    "\"sits\"  \n",
    "\n",
    "\"on\" \n",
    "\n",
    "\"the\" \n",
    "\n",
    "\"porch\"  \n",
    "\n",
    "\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455861b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Creating tokens:**\n",
    "\n",
    "<br>\n",
    "\n",
    "* can be problematic\n",
    "<br>\n",
    "\n",
    "    * some languages don't use empty spaces, and using punctuation marks is problematic for defining tokens because of their ambiguity\n",
    "    \n",
    "<br>\n",
    "\n",
    "* sometimes text needs to be preprocessed before tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf777c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**`Stop words`**\n",
    "\n",
    "* tokens that appear most often\n",
    "    <br>\n",
    "    \n",
    "    * e.g. as, of, the, etc.\n",
    "    \n",
    "    \n",
    "* these words tipically provide no context, so they are usually removed to improve performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a047df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* a list of commonly removed words is called a **`stop word list`**\n",
    "\n",
    "\n",
    "* **IMPORTANT:** always convert all of your text to lowercase before trying to remove stopwords\n",
    "<br>\n",
    "    \n",
    "    * **stopwords corpora** are all in lowercase so you might miss some word if you run into the uppercase version of it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67225eba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example before removing stop words:**\n",
    "\n",
    "\"The\"  , \"dog\"  , \"sits\"  , \"on\"  , \"the\" , \"porch\"  ,  \".\"\n",
    "\n",
    "**Example after removing stopwords:**\n",
    "\n",
    "\"The\", \"dog\", \"sits\", \"porch\", \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854be26c",
   "metadata": {},
   "source": [
    "* we can also remove punctuation marks, special characters and numbers if we want\n",
    "    <br>\n",
    "    \n",
    "    * modify the stop words corpus so that it includes those values, so they get removed when we remove stop words (do this before you remove stopwords)\n",
    "    \n",
    "    * leverage the power of **`RegEx`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2131bef",
   "metadata": {},
   "source": [
    "# Tokenizing text into words using `NLTK`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca3f3f",
   "metadata": {},
   "source": [
    "* **`NLTK`** uses **regular expressions** to tokenize text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9496dc58",
   "metadata": {},
   "source": [
    "* we use the **`word_tokenize()`** method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bae762",
   "metadata": {},
   "source": [
    "* example sentence that we need to tokenize : **\"The dog sits on the porch.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878ad2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e378c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library from nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975086eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example text data\n",
    "\n",
    "text = \"The dog sits on the porch.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "97c915b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swaheed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30dd9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tokenization\n",
    "\n",
    "tokenized_text = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1242018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'sits', 'on', 'the', 'porch', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a811cc",
   "metadata": {},
   "source": [
    "**`RegexpTokenizer`**\n",
    "\n",
    "* a lot more flexible than the standard **`word_tokenize()`** method (e.g. allows us to exclude punctuation marks) \n",
    "\n",
    "\n",
    "* allows us to precisely define how we want to tokenize the text\n",
    "\n",
    "\n",
    "* **however, you must be familiar with regex**\n",
    "    * checkout the bonus <a href=\"Bonus_material_RegEx.ipynb\">Bonus_material_RegEx.ipynb</a> notebook if you need a refresher on Python regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c009ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b07c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer parameters\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70d3d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text data\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a485c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'dog', 'sits', 'on', 'the', 'porch']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92a983",
   "metadata": {},
   "source": [
    "**`ToktokTokenizer`**\n",
    "\n",
    "* much faster than the default tokenizer\n",
    "\n",
    "\n",
    "* tested and proven for English, Persian, Russian, Czech, French, German and a few other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77a9d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the brown corpus \n",
    "# Import the time module \n",
    "\n",
    "from nltk.corpus import brown\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69d210eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\swaheed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc2acd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard tokenization on brown corpus (corpus of 1.4 million words)\n",
    "\n",
    "brown_corpus = brown.raw()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tokenized_text = word_tokenize(brown_corpus)\n",
    "\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b842e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.96935486793518\n"
     ]
    }
   ],
   "source": [
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15eb87ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Toktok tokenizer\n",
    "\n",
    "from nltk.tokenize import ToktokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33bd49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_tok_tokenizer = ToktokTokenizer()\n",
    "tokenized_text = tok_tok_tokenizer.tokenize(\"this is an example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94b6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toktok tokenization on brown corpus (corpus of 1.4 million words)\n",
    "\n",
    "brown_corpus = brown.raw()\n",
    "\n",
    "tok_tok_tokenizer = ToktokTokenizer()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tokenized_text = tok_tok_tokenizer.tokenize(brown_corpus)\n",
    "\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c6b3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1360223293304443\n"
     ]
    }
   ],
   "source": [
    "print(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b7c0c",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f2269",
   "metadata": {},
   "source": [
    "**Tokenize the following sentence while also removing punctuation marks:** \n",
    "\n",
    "```\n",
    "\"Do not go where the path may lead, go instead where there is no path and leave a trail.\"\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de2a28",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee36a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'not', 'go', 'where', 'the', 'path', 'may', 'lead', 'go', 'instead', 'where', 'there', 'is', 'no', 'path', 'and', 'leave', 'a', 'trail']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "text = \"Do not go where the path may lead, go instead where there is no path and leave a trail.\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0056c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e2dd9",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "**Tokenize the following text using the fastest tokenizer from `NLTK`:**\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "Our knowledge has made us cynincal.\n",
    "Our cleverness, hard and unkind.\n",
    "We think too much, and feel too little.\n",
    "More than machinery, we need humanity.\n",
    "More that cleverness, we need kindness and gentleness.\n",
    "Without these qualities life will be violent, and all will be lost.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cf99d",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dafa430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'knowledge', 'has', 'made', 'us', 'cynincal.', 'Our', 'cleverness', ',', 'hard', 'and', 'unkind.', 'We', 'think', 'too', 'much', ',', 'and', 'feel', 'too', 'little.', 'More', 'than', 'machinery', ',', 'we', 'need', 'humanity.', 'More', 'that', 'cleverness', ',', 'we', 'need', 'kindness', 'and', 'gentleness.', 'Without', 'these', 'qualities', 'life', 'will', 'be', 'violent', ',', 'and', 'all', 'will', 'be', 'lost', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "text = \"\"\"\n",
    "Our knowledge has made us cynincal.\n",
    "Our cleverness, hard and unkind.\n",
    "We think too much, and feel too little.\n",
    "More than machinery, we need humanity.\n",
    "More that cleverness, we need kindness and gentleness.\n",
    "Without these qualities life will be violent, and all will be lost.\n",
    "\"\"\"\n",
    "tokenized_text = tok_tok_tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74f04e",
   "metadata": {},
   "source": [
    "# Removing stopwords using `NLTK`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0542cc",
   "metadata": {},
   "source": [
    "* **`NLTK`** has a corpus of stopwords included into it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f4ba1",
   "metadata": {},
   "source": [
    "* we can use that corpus, but we can also add extra words it if we want to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72db50",
   "metadata": {},
   "source": [
    "* **IMPORTANT: remember to lowercase your text, otherwise, some stopwords may be missed!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69142f79",
   "metadata": {},
   "source": [
    "* let's demonstrate on the following sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc4855",
   "metadata": {},
   "source": [
    "**\"Our virtues and our failings are inseparable, like force and matter. When they separate, man is no more.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa4cf9",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "546a9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stopwords list\n",
    "# and word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a33ac078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swaheed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67fe4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c7dba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "185df29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example text data\n",
    "\n",
    "text = \"Our virtues and our failings are inseparable, like force and matter. When they separate, man is no more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cadc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "\n",
    "tokenized_text = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26ded2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop that removes stopwords\n",
    "# and store the result in a new list\n",
    "\n",
    "text_stopwords_removed = []\n",
    "\n",
    "for word in tokenized_text:\n",
    "    if word not in stop_words:\n",
    "        text_stopwords_removed.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "863ae868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virtues', 'failings', 'inseparable', ',', 'like', 'force', 'matter', '.', 'separate', ',', 'man', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text_stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a76209",
   "metadata": {},
   "source": [
    "**Better solution:**\n",
    "\n",
    "* instead of using loops, make your code reusable - use functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f9d894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that removes stopwords\n",
    "# and also lets users pick the stop words corpus\n",
    "\n",
    "def remove_stopwords(text, language):\n",
    "    stop_words = stopwords.words(language)\n",
    "    return [w for w in text if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1baed211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virtues', 'failings', 'inseparable', ',', 'like', 'force', 'matter', '.', 'separate', ',', 'man', '.']\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(tokenized_text, language=\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f83156",
   "metadata": {},
   "source": [
    "**If we want to add something to the corpus (e.g. punctuation) we just need to add it to the list of stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b93039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to the stopwords list\n",
    "\n",
    "stop_words.extend([\".\", \",\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c49de2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "\n",
    "text_stopwords_removed = []\n",
    "\n",
    "for word in tokenized_text:\n",
    "    if word not in stop_words:\n",
    "        text_stopwords_removed.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "decbc514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virtues', 'failings', 'inseparable', 'like', 'force', 'matter', 'separate', 'man']\n"
     ]
    }
   ],
   "source": [
    "print(text_stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a33dd",
   "metadata": {},
   "source": [
    "**NOTE:** Sometimes you might want to combine different corpora of stopwords together. If you do that, be sure to increase the efficiency of your code by getting rid of the duplicates (use a set instead of a list for removing stopwords)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7648a86",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5512d8b",
   "metadata": {},
   "source": [
    "**Create a function that removes all stop words and punctuation marks from a sentence. Use it on the following sentence:**\n",
    "\n",
    "```\n",
    "\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d66b6",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e98a16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_s(text):\n",
    "    tokenized_text = tok_tok_tokenizer.tokenize(text)\n",
    "    stop_words.extend(\"'\")\n",
    "    return [w for w in tokenized_text  if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b19339da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'set',\n",
       " 'goals',\n",
       " 'ridiculously',\n",
       " 'high',\n",
       " 'failure',\n",
       " 'fail',\n",
       " 'everyone',\n",
       " 'else',\n",
       " 'success']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_is = \"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "remove_s(text_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e1b2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Sentence level representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d7c72",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* before analyzing each sentence separately, we often need to separate a large chunk of text into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1bcd1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* similar procedure to how we separate words inside a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c665e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* let's tokenize the following text into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea8cf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48229588",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* let's tokenize the following text into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f8ef3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**\"Machine Learning can usually be divided into classic Machine Learning and Deep Learning. Classic Machine Learning is easier to understand. Deep Learning on the other hand is a bit more complex.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a61465",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the result we will get by tokenizing it into sentences will look like this\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fb800",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**['Machine Learning can usually be divided into classic Machine Learning and Deep Learning.',\n",
    "'Classic Machine Learning is easier to understand.',\n",
    "'Deep Learning on the other hand is a bit more complex.']**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d0bce",
   "metadata": {},
   "source": [
    "## Separating sentences into `n-grams`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8d31f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* grasping grammar or context from single tokens (especially when they are just words) is very hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9386eb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* that is a very common problem when working with **`bag-of-words`** models with **`unigrams` (one phrase = one token)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bcbc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* solution: **work with phrases that contain multiple tokens (`n-grams`)**\n",
    "    <br>\n",
    "    \n",
    "    * **NOTE:** using n-grams where n is a large number is rare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0b49c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example of a `bigram` (one phrase = two tokens):**\n",
    "\n",
    "* \"metal boat\" is not the same as \"boat metal\"\n",
    "\n",
    "\n",
    "* looking at a bigram allows us to capture that distinction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75352824",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we can even work with whole sentences, however using very big **`n-grams`** is not recommended (we quickly start working with whole sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773269f",
   "metadata": {},
   "source": [
    "## Tokenizing documents into sentences using `NLTK`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744199dd",
   "metadata": {},
   "source": [
    "* we use the **`sent_tokenize()`** method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318cb1c",
   "metadata": {},
   "source": [
    "**Let's use the following text for the purposes of demonstration.**\n",
    "\n",
    "\"Machine Learning can usually be divided into classic Machine Learning and Deep Learning. Classic Machine Learning is relatively easy to understand. Deep Learning on the other hand is a bit more complex.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88111c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sent_tokenize method\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1745d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example text data\n",
    "\n",
    "text = \"\"\"Machine Learning can usually be divided into classic Machine Learning and Deep Learning. \n",
    "Classic Machine Learning is relatively easy to understand. Deep Learning on the other hand is a bit more complex.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8099dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text data\n",
    "\n",
    "tokenized_text = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e6d206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine Learning can usually be divided into classic Machine Learning and Deep Learning.', 'Classic Machine Learning is relatively easy to understand.', 'Deep Learning on the other hand is a bit more complex.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5630d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Separating sentences into `n-grams` in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2d543",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* to separate sentences into `n-grams` in NLTK, we use the `ngrams()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e24db85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the ngrams method\n",
    "\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c16d4232",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create an example sentence\n",
    "# The sentence must be tokenized\n",
    "\n",
    "example_sentence = [\"Deep\", \"Learning\", \"on\", \"the\", \"other\", \"hand\", \"is\", \"a\", \"bit\", \"more\", \"complex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca8bc32a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define n\n",
    "\n",
    "n = 3\n",
    "\n",
    "# Separate sentence into trigrams\n",
    "\n",
    "trigrams = ngrams(example_sentence, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e105a8c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd4b4a25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Store trigrams inside a list\n",
    "\n",
    "trigram_list = []\n",
    "\n",
    "for phrase in trigrams:\n",
    "    trigram_list.append(phrase)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dc8ea599",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Deep', 'Learning', 'on'),\n",
       " ('Learning', 'on', 'the'),\n",
       " ('on', 'the', 'other'),\n",
       " ('the', 'other', 'hand'),\n",
       " ('other', 'hand', 'is'),\n",
       " ('hand', 'is', 'a'),\n",
       " ('is', 'a', 'bit'),\n",
       " ('a', 'bit', 'more'),\n",
       " ('bit', 'more', 'complex')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the list of trigrams\n",
    "\n",
    "trigram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac21e63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Document level representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64251a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* a document can be any text file (log file, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640db6e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we can use sparse or dense representations for the text in a document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdab00",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* we can represent a document (or a set of documents) as a **`document-term matrix`**\n",
    "    <br>\n",
    "    \n",
    "    * **columns = tokens**\n",
    "    <br>\n",
    "    \n",
    "    * **rows = documents in the set of documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95eb7c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Two common representations (ways of creating the document-term matrix):**\n",
    "    <br>\n",
    "    \n",
    "   * `Bag-of-words`\n",
    "   <br>\n",
    "    \n",
    "   * `TFIDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af5ba6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Bag-of-words`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb35160",
   "metadata": {},
   "source": [
    "<img src=\"https://edlitera-images.s3.amazonaws.com/bag_of_words_example_image.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b44b5a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* text is represented as a **list of counts of its unique words**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee42ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* converting a set of documents into a **`document-term matrix`** where we define each element as equal to the number of times it occurs in a document\n",
    "    * this procedure is known as **`count vectorization`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb0011",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* used for feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ad7ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* very important to include a **stop word filter** as preprocessing\n",
    "    * since words are weighed based on how many times they appear, stop words would otherwise gain too much importance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e471dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* one very big problem with this method: sometimes even the words that appear rarely in a document can be important\n",
    "    * we will address this in a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9b8dc",
   "metadata": {},
   "source": [
    "# `Bag-of-words` using `Scikit-Learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379adc6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* typically **count-vectorization** can be done using the **`Scikit-Learn`** library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49eae4",
   "metadata": {},
   "source": [
    "* we will take a look at an example now, but we will leave explaining the **`Scikit-Learn`** library in detail for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99843337",
   "metadata": {},
   "source": [
    "* `pandas` (https://pandas.pydata.org/) is an excellent package for doing data processing in Python\n",
    "\n",
    "\n",
    "* we teach an entire course on it, if you're interested in more in-depth Python data processing!\n",
    "\n",
    "\n",
    "* a pandas `DataFrame` is a container for storing tabular data\n",
    "    * it's like an Excel sheet, with rows and columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989cb1c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa0bcaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a43f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example text data\n",
    "\n",
    "texts = [\"good movie\", \"bad movie\", \"did not like it\", \"good one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "204d852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer\n",
    "\n",
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e196cdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply vectorizer to data\n",
    "\n",
    "features = count.fit_transform(texts)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd23a179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a sparse matrix into a dense matrix\n",
    "\n",
    "dense_features = features.todense()\n",
    "\n",
    "dense_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b14d595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'did', 'good', 'it', 'like', 'movie', 'not', 'one']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get column names\n",
    "# Columns are our features\n",
    "\n",
    "column_names = count.get_feature_names()\n",
    "\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7723c613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>did</th>\n",
       "      <th>good</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good movie</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad movie</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>did not like it</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good one</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bad  did  good  it  like  movie  not  one\n",
       "good movie         0    0     1   0     0      1    0    0\n",
       "bad movie          1    0     0   0     0      1    0    0\n",
       "did not like it    0    1     0   1     1      0    1    0\n",
       "good one           0    0     1   0     0      0    0    1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Pandas dataframe to show result\n",
    "\n",
    "pd.DataFrame(\n",
    "    features.todense(), \n",
    "    columns=count.get_feature_names(), \n",
    "    index=texts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21130fcc",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a5573",
   "metadata": {},
   "source": [
    "**Create a `Bag-of-words` representation of the following two sentences using a tokenizer from `NLTK`:**\n",
    "\n",
    "```\n",
    "string_1 = \"Let us demonstrate the bag of words technique.\"\n",
    "\n",
    "string_2 = \"And the TFIDF technique.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31567d",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af5af425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             bag  demonstrate  let  technique  us  words\n",
      "let            0            0    1          0   0      0\n",
      "us             0            0    0          0   1      0\n",
      "demonstrate    0            1    0          0   0      0\n",
      "bag            1            0    0          0   0      0\n",
      "words          0            0    0          0   0      1\n",
      "technique      0            0    0          1   0      0\n",
      "           technique  tfidf\n",
      "tfidf              0      1\n",
      "technique          1      0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def gen_token(text):\n",
    "    #text = \"Let us demonstrate the bag of words technique.\"\n",
    "    #ext = text.split(' ')\n",
    "    text = word_tokenize(text.lower())\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "\n",
    "    count = CountVectorizer()\n",
    "    features = count.fit_transform(text)\n",
    "\n",
    "    dense_features = features.todense()\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        features.todense(), \n",
    "        columns=count.get_feature_names(), \n",
    "        index=text)\n",
    "    return df\n",
    "    \n",
    "text1 = \"Let us demonstrate the bag of words technique.\"\n",
    "text2 = \"And the TFIDF technique.\"\n",
    "\n",
    "df1 = gen_token(text1)\n",
    "print(df1)\n",
    "df2 = gen_token(text2)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c2b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `TFIDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edd989",
   "metadata": {},
   "source": [
    "<img src=\"https://edlitera-images.s3.amazonaws.com/TFIDF_example_image.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69884725",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* solves the main problem of the **`Bag-of-words`** method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d584ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* rare words are given greater weigth by multiplying the **`term frequency (TF)`** with the **`inverse document frequency (IDF)`** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31a7fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **`term frequency (TF)`** - how many times a word appears in some document\n",
    "\n",
    "    <br>\n",
    "    \n",
    "    * the **`TF`** factor increases the **`TFIDF`** value of a token proportionally to the number of time a term occurs in the set of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923c97e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **`inverse document frequency (IDF)`** - the inverse of **`document frequency`**\n",
    "    <br>\n",
    "    \n",
    "    * without going into math, the idea behind **`IDF`** is that the terms that appear more frequently in a collection of documents are less informative than those that appear less often\n",
    "    <br>\n",
    "    \n",
    "    * this part reduces the **`TFIDF`** value of a token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9e7b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**This is THE most popular weighting method for creating document matrices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685198a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* usually performed using the **`Scikit-Learn`** library (https://scikit-learn.org/stable/)\n",
    "    * we will take a look at an example now, but we will go over the **`Scikit-Learn`** library a bit later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2de8dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "270b2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "159b0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example text data\n",
    "\n",
    "texts = [\"good movie\", \"bad movie\", \"did not like it\", \"good one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55d10c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b2a3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vectorizer to data\n",
    "\n",
    "features = tfidf.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55d85aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>did</th>\n",
       "      <th>good</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "      <th>one</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good movie</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad movie</th>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>did not like it</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good one</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bad  did      good   it  like     movie  not       one\n",
       "good movie       0.000000  0.0  0.707107  0.0   0.0  0.707107  0.0  0.000000\n",
       "bad movie        0.785288  0.0  0.000000  0.0   0.0  0.619130  0.0  0.000000\n",
       "did not like it  0.000000  0.5  0.000000  0.5   0.5  0.000000  0.5  0.000000\n",
       "good one         0.000000  0.0  0.619130  0.0   0.0  0.000000  0.0  0.785288"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Pandas dataframe to demonstrate result\n",
    "\n",
    "pd.DataFrame(\n",
    "    features.todense(), \n",
    "    columns=tfidf.get_feature_names(),\n",
    "    index=texts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529114d4",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd3898",
   "metadata": {},
   "source": [
    "**Create a  `TFIDF` representation of the first two sentences of the following string using a tokenizer from `NLTK`:**\n",
    "\n",
    "```\n",
    "text = \"I drive a red car. Bill drives a blue car. Jack drives a yellow car. \"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5846a0",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6260a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i drive a red car.', 'i want to drive a blue car.']\n",
      "[[0.         0.50154891 0.50154891 0.70490949 0.         0.        ]\n",
      " [0.49922133 0.35520009 0.35520009 0.         0.49922133 0.49922133]]\n",
      "['blue', 'car', 'drive', 'red', 'to', 'want']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>car</th>\n",
       "      <th>drive</th>\n",
       "      <th>red</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i drive a red car.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i want to drive a blue car.</th>\n",
       "      <td>0.499221</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499221</td>\n",
       "      <td>0.499221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 blue       car     drive       red        to  \\\n",
       "i drive a red car.           0.000000  0.501549  0.501549  0.704909  0.000000   \n",
       "i want to drive a blue car.  0.499221  0.355200  0.355200  0.000000  0.499221   \n",
       "\n",
       "                                 want  \n",
       "i drive a red car.           0.000000  \n",
       "i want to drive a blue car.  0.499221  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "def gen_token(text):\n",
    "    #text = \"Let us demonstrate the bag of words technique.\"\n",
    "    #ext = text.split(' ')\n",
    "    text = sent_tokenize(text.lower())\n",
    "    text = text[0:2]\n",
    "    #text = [w for w in text if w not in stop_words]\n",
    "    print(text)\n",
    "    tfidf = TfidfVectorizer()\n",
    "    features = tfidf.fit_transform(text)\n",
    "\n",
    "    dense_features = features.todense()\n",
    "    print(dense_features)\n",
    "    print(tfidf.get_feature_names())\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        features.todense(), \n",
    "        columns= tfidf.get_feature_names(), \n",
    "        index=text)\n",
    "    return df\n",
    "text = \"I drive a red car. I want to drive a blue car. Jack drives a yellow car. \"\n",
    "gen_token(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34733e7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Contractions`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4db730",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* very important when working with text in English\n",
    "    * text often contains contractions, and most Python libraries won't know how to deal with them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef261d8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* using something like **`RegEx`** allows us to deal with that problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb78212",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* in real world applications, users will often have a good idea of whether their code will run into contractions or not (and also what are some potential contractions it might run into)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabf0bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* other languages might have their own, similar quirks so it is always a good idea to include some **`RegEx`** into  preprocessing to deal with certain parts of text you know your model will have a hard time working with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41882143",
   "metadata": {},
   "source": [
    "* if you need a refresher on **`RegEx`**, take a look at the **`RegEx`** notebook in the bonus material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d18e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5a884f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "\n",
    "import re\n",
    "\n",
    "# Create function that removes contractions\n",
    "\n",
    "def decontracted(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"\\'all\", \"ou all\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5604ab63",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create example text data\n",
    "\n",
    "text = \"I'm finding it hard to believe that wasn't her real name. I simply can't believe it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ed914d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am finding it hard to believe that was not her real name. I simply can not believe it.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display result\n",
    "\n",
    "decontracted(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38386208",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d3908",
   "metadata": {},
   "source": [
    "**Create a function that will get rid of the following contractions:**\n",
    "    \n",
    "* Y'all\n",
    "* I'm\n",
    "* didn't\n",
    "\n",
    "**Apply that function on the following sentenceand print the result:** \n",
    "\n",
    "```\n",
    "\"Y'all should know I'm quite sure you didn't complete the project yet.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5d46e",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ba1846b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1= \"Y'all\"\n",
    "t2 = \"I'm\"\n",
    "t3 = \"didn't\"\n",
    "decontracted(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b0758a",
   "metadata": {},
   "source": [
    "# `Lexical Characteristics Cheat Sheet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a084d5",
   "metadata": {},
   "source": [
    "* converting text into base word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e4fd19",
   "metadata": {},
   "source": [
    "**Different types of representations:**\n",
    "   <br>\n",
    "   \n",
    "   * `word level representations`\n",
    "     <br>\n",
    "   \n",
    "   * `sentence level representations`\n",
    "     <br>\n",
    "   \n",
    "   * `document level representations`\n",
    "     <br>\n",
    "   \n",
    "   * `dense representations`\n",
    "     <br>\n",
    "   \n",
    "   * `embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd33b6",
   "metadata": {},
   "source": [
    "### `Word level representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017f7e9",
   "metadata": {},
   "source": [
    "**`Word level tokenization`**\n",
    "\n",
    "* separating text into elements that hold some meaning (words when talking about word level representations)\n",
    "\n",
    "\n",
    "* be careful: \n",
    "    * try to remove stop words, punctuation and special symbols before tokenizing\n",
    "    * simple procedure: load a corpus of stopwords and remove every word from your text that is present in your stop words corpus\n",
    "\n",
    "\n",
    "* performed using tokenizers\n",
    "    * in NLTK we tipically use the **`default tokenizer`**, the **`TokTok Tokenizer`** and the **`Regexp Tokenizer`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8919bb",
   "metadata": {},
   "source": [
    "### `Sentence level representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd6163",
   "metadata": {},
   "source": [
    "**`Sentence level tokenization`**\n",
    "\n",
    "* splitting larger text into smaller components (e.g. sentences)\n",
    "\n",
    "\n",
    "* be careful: \n",
    "    * try to remove stop words, punctuation and special symbols before tokenizing\n",
    "    * simple procedure: load a corpus of stopwords and remove every word from your text that is present in your stop words corpus\n",
    "\n",
    "\n",
    "* performed using the **`sent_tokenize()`** method from **`NLTK`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c6fc0",
   "metadata": {},
   "source": [
    "### `Document level representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b86c46",
   "metadata": {},
   "source": [
    "* we can represent some document (or a set of documents) as a **`document-term matrix`**\n",
    "    <br>\n",
    "    \n",
    "    * **columns = documents in the set of documents**\n",
    "    <br>\n",
    "    \n",
    "    * **rows = tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b90333",
   "metadata": {},
   "source": [
    "**Two common representations (ways of creating the document-term matrix):**\n",
    "    <br>\n",
    "    \n",
    "   * `Bag-of-words`\n",
    "   <br>\n",
    "    \n",
    "   * `TFIDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0a943",
   "metadata": {},
   "source": [
    "**`Bag-of-words`**\n",
    "\n",
    "* converting a set of documents into a **`document-term matrix`** where we define each element as equal to the number of times it occures in some document\n",
    "\n",
    "\n",
    "* not very good because it ignores the importance of rare words\n",
    "\n",
    "\n",
    "* implementation - **`CountVectorizer`** from **`Scikit-learn`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e41c45",
   "metadata": {},
   "source": [
    "**`TFIDF`**\n",
    "\n",
    "* gives rare words greater weight by multiplying the **`term frequency (TF)`** of the document with the **`inverse document frequency (IDF)`** \n",
    "\n",
    "\n",
    "* **`TF`** - how many times a word appears in some document\n",
    "\n",
    "\n",
    "* **`IDF`** - the terms that appear more frequently in a collection of documents are less informative than those that appear less often\n",
    "\n",
    "\n",
    "* implementation - **`TfidfVectorizer`** from **`Scikit-learn`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6a2b9",
   "metadata": {},
   "source": [
    "**Bonus - `Contractions`**\n",
    "\n",
    "* removing contractions is important for English\n",
    "    * do this before tokenizing your data\n",
    "\n",
    "\n",
    "* if you want to get rid of contractions:\n",
    "    * create a function using **`RegEx`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f46713",
   "metadata": {},
   "source": [
    " <div>\n",
    "<img src=\"https://edlitera-images.s3.amazonaws.com/new_edlitera_logo.png\" width=\"500\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
